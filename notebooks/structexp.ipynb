{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n"]}],"source":["print(1)\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/scratch/2469038g/miniconda3/envs/ryanenv2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import ir_datasets\n","import email\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n","import gc\n","from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n","from config import *\n","import re\n","import numpy as np"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Data set-up"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def get_sara():\n","    return ir_datasets.load('sara')\n","\n","\n","def dataset_to_df(sara_dataset):\n","    doc_ids = []\n","    doc_text = []\n","    doc_sens = []\n","    for doc in sara_dataset.docs_iter():\n","        doc_ids.append(doc.doc_id)\n","        doc_text.append(doc.text)\n","        doc_sens.append(doc.sensitivity)\n","\n","    sara_dict = {'doc_id':doc_ids, 'text':doc_text, 'sensitivity':doc_sens}\n","    df = pd.DataFrame.from_dict(sara_dict)\n","    return df\n","\n","\n","def get_sample_n(data, n):\n","    return data.sample(n=n, random_state=1)\n","\n","def get_sample_frac(data, frac):\n","    return data.sample(frac=frac, random_state=1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Pre-processing"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Preprocessing 1 - Separating replies using Original Message and Forwarded tags."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def proc1(dataset):\n","    def get_orig(message, find_orig_message):\n","        #line_start = message.rfind('\\n', 0, find_orig_message)\n","        line_start = find_orig_message\n","        base_message = message[:line_start]\n","        line_end = find_orig_message + 5 #message.find('\\n', find_orig_message)\n","        orig_message = message[line_end:]\n","        return base_message, orig_message\n","\n","    def get_separate_messages(message):\n","        payload = message.split('\\r\\n\\r\\n')[1]\n","        separate_messages = []\n","\n","        find_fwd_message = payload.find('\\n----- Forw')\n","        find_orig_message = payload.find('-----Original Message-')\n","        \n","        n = payload\n","        while (find_fwd_message > 0 or find_orig_message > 0):\n","            if (find_fwd_message > 0 and find_orig_message > 0):\n","                if find_fwd_message < find_orig_message:\n","                    b, n = get_orig(n, find_fwd_message)\n","                else:\n","                    b, n = get_orig(n, find_orig_message)\n","            elif (find_fwd_message > 0 and find_orig_message < 0):\n","                b, n = get_orig(n, find_fwd_message)\n","            else:\n","                b, n = get_orig(n, find_orig_message)\n","            \n","            find_fwd_message = n.find('\\n----- Forw')\n","            find_orig_message = n.find('-----Original Message-')\n","\n","            b = re.sub(r'\\s+', ' ', b)\n","            separate_messages.append(b)\n","\n","        n = re.sub(r'\\s+', ' ', n)\n","        separate_messages.append(n)\n","\n","        return separate_messages\n","\n","    def preprocessing_dataframe(testing_sample):\n","        ids = []\n","        texts = []\n","        sens = []\n","        preproc = {}\n","\n","        for s in testing_sample.iterrows():\n","            separate_messages = get_separate_messages(s[1].text)\n","\n","            if len(separate_messages) == 1:\n","                ids.append(s[1].doc_id)\n","                texts.append(separate_messages[0])\n","                sens.append(s[1].sensitivity)\n","            else:\n","                for i, m in enumerate(separate_messages):\n","                    id_part = s[1].doc_id + '_' + str(i)\n","                    ids.append(id_part)\n","                    texts.append(m)\n","                    sens.append(s[1].sensitivity)\n","\n","        preproc['doc_id'] = ids\n","        preproc['text'] = texts\n","        preproc['sensitivity'] = sens\n","        return preproc\n","\n","    def get_preprocessed_sara_replies(dataset):\n","        preproc = preprocessing_dataframe(dataset)\n","        preproc_df = pd.DataFrame.from_dict(preproc)\n","        return preproc_df\n","\n","    return get_preprocessed_sara_replies(dataset)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Preprocessing 2 - Separating paragraphs. Each paragraph is treated as a separate document. \n","- Where double newline exists. Also found some areas with '\\n whitespace \\n' that has been replaced.\n","- As reply headers cannot be cleaned easily, the paragraph with To for example could be removed as this should just be the header. Likewise with empty documents. \n","- The smallest ~1000 documents have length less than 20: these are usually letters/punctuation/name sign offs - these are less informative or useful in identifying the sensitivity of the information, so have been removed for simplicity in this preprocessing strategy."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def proc2(dataset):\n","    def process_text(input_text):\n","        clean = input_text\n","        clean = re.sub(r'=20', ' ', clean)\n","        clean = re.sub(r'=09', ' ', clean)\n","        # Also > Whitespace1+ > \n","        clean = re.sub(r'\\n\\>', '\\n', clean)\n","        clean = re.sub(r'\\n\\n\\t', '\\n', clean)\n","        # Replace whitespace between two newline characters with a single newline\n","        #cleaned_text = re.sub(r'\\n\\s*\\n', '\\n\\n', input_text)\n","        #clean = re.sub(r'\\n \\n', '\\n\\n', clean)\n","        clean = re.sub(r'\\n \\n', '\\n', clean)\n","        return clean\n","\n","    def get_separate_paras(doc):\n","        paras = []\n","        payload = doc.split('\\r\\n\\r\\n')[1]\n","        payload = process_text(payload)\n","        split_load = payload.split('\\n\\n')\n","        for m in split_load:\n","            #if 'Subject:' in t:\n","            #    continue\n","            if 'To:' in m or 'From:' in m or 'mailto' in m or m == '':\n","                continue\n","            if len(m) < 2:\n","                continue\n","            if len(m) < 20:\n","                continue\n","            \n","            m = re.sub(r'\\s+', ' ', m)\n","            paras.append(m)\n","        return paras\n","\n","    def preproc(sample):\n","        sep_docs = []\n","        sep_docs_ids = []\n","        sep_docs_sens = []\n","        i = 0\n","        for s in sample.iterrows():\n","\n","            separate_messages = get_separate_paras(s[1].text)\n","            for i, m in enumerate(separate_messages):\n","                id_part = s[1].doc_id + '_' + str(i)\n","                sep_docs_ids.append(id_part)\n","                sep_docs.append(m)\n","                sep_docs_sens.append(s[1].sensitivity)\n","\n","        preproc = {}\n","        preproc['doc_id'] = sep_docs_ids\n","        preproc['text'] = sep_docs\n","        preproc['sensitivity'] = sep_docs_sens\n","        preproc_df = pd.DataFrame.from_dict(preproc)\n","        return preproc_df\n","\n","    def get_preprocessed_sara_paras(dataset):\n","        preproc_data = preproc(dataset)\n","        preproc_df = pd.DataFrame.from_dict(preproc_data)\n","        return preproc_df\n","\n","    return get_preprocessed_sara_paras(dataset)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Preprocessing 3"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def proc3(dataset):\n","    def get_orig(message, find_orig_message):\n","        #line_start = message.rfind('\\n', 0, find_orig_message)\n","        line_start = find_orig_message\n","        base_message = message[:line_start]\n","        line_end = find_orig_message + 5 #message.find('\\n', find_orig_message)\n","        orig_message = message[line_end:]\n","        if 'Subject' in orig_message:\n","            over_header = orig_message.find('Subject:')\n","            orig_message = orig_message[over_header:]\n","        return base_message, orig_message\n","\n","    def get_separate_messages(message):\n","        payload = message.split('\\r\\n\\r\\n')[1]\n","        separate_messages = []\n","\n","        find_fwd_message = payload.find('- Forw')\n","        find_orig_message = payload.find('-----Original Message-')\n","        \n","        n = payload\n","        while (find_fwd_message > 0 or find_orig_message > 0):\n","            if (find_fwd_message > 0 and find_orig_message > 0):\n","                if find_fwd_message < find_orig_message:\n","                    b, n = get_orig(n, find_fwd_message)\n","                else:\n","                    b, n = get_orig(n, find_orig_message)\n","            elif (find_fwd_message > 0 and find_orig_message < 0):\n","                b, n = get_orig(n, find_fwd_message)\n","            else:\n","                b, n = get_orig(n, find_orig_message)\n","            \n","            find_fwd_message = n.find('- Forw')\n","            find_orig_message = n.find('-----Original Message-')\n","\n","            b = re.sub(r'\\s+', ' ', b)\n","            if len(b) < 20:\n","                continue\n","            separate_messages.append(b)\n","\n","        n = re.sub(r'\\s+', ' ', n)\n","        if 'Subject' in n:\n","            over_header = n.find('Subject:')\n","            n = n[over_header:]\n","        if len(n) > 20:\n","            separate_messages.append(n)\n","\n","        return separate_messages\n","\n","    def preprocessing_dataframe(testing_sample):\n","        ids = []\n","        texts = []\n","        sens = []\n","        preproc = {}\n","\n","        for s in testing_sample.iterrows():\n","            separate_messages = get_separate_messages(s[1].text)\n","\n","            if len(separate_messages) == 1:\n","                ids.append(s[1].doc_id)\n","                texts.append(separate_messages[0])\n","                sens.append(s[1].sensitivity)\n","            else:\n","                for i, m in enumerate(separate_messages):\n","                    id_part = s[1].doc_id + '_' + str(i)\n","                    ids.append(id_part)\n","                    texts.append(m)\n","                    sens.append(s[1].sensitivity)\n","\n","        preproc['doc_id'] = ids\n","        preproc['text'] = texts\n","        preproc['sensitivity'] = sens\n","        return preproc\n","\n","    def get_preprocessed_sara_p3(dataset):\n","        preproc = preprocessing_dataframe(dataset)\n","        preproc_df = pd.DataFrame.from_dict(preproc)\n","        return preproc_df\n","\n","    return get_preprocessed_sara_p3(dataset)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Llama 2 with SARA"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def get_llama2(name):\n","    access_token = l2_token\n","    model_name = name\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, token=access_token)\n","    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", token=access_token, cache_dir=my_cache, revision='gptq-8bit-64g-actorder_True')\n","    return tokenizer, model\n","\n","def get_model_version(m, v):\n","    models_dict = {'get_llama2' : get_llama2}\n","    return models_dict.get(m)(v)\n"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["def llm_inference(document, prompt, model, tokenizer):\n","  inputs = tokenizer(prompt(document), return_tensors='pt')\n","  generation_config = GenerationConfig(\n","    # Unable to set temperature to 0 - https://github.com/facebookresearch/llama/issues/687 - use do_sample=False for greedy decoding\n","    do_sample=False,\n","    max_new_tokens=20,\n","  )\n","  output = model.generate(inputs=inputs.input_ids.cuda(), attention_mask=inputs.attention_mask.cuda(), generation_config=generation_config)\n","  return tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","def display_gen_text(output, prompt_end):\n","  end_template = output.find(prompt_end)\n","  return output[end_template:]\n","\n","def prompt_to_reply(d, p, model, tokenizer, end):\n","  response = llm_inference(d, p, model, tokenizer)\n","  return display_gen_text(response, end)\n","\n","\n","# String matching on model response\n","def post_process_classification(classification, ground_truth):\n","    if 'non-sensitive' in classification.lower():\n","        if ground_truth == 0:\n","            return 'TN', 0\n","        else:\n","            return 'FN', 0\n","\n","    elif 'sensitive' in classification.lower() and 'non-sensitive' not in classification.lower():\n","        if ground_truth == 1:\n","            return 'TP', 1\n","        else:\n","            return 'FP', 1\n","\n","    else:\n","        # Further processing required\n","        return classification, None\n","        further_processing_required[sample[1].doc_id] = classification\n","\n","\n","def clear_memory():\n","    # Prevents cuda out of memory\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","\n","# Dataset - dataframe, prompt_strategy - prompt function name\n","def llm_experiment(dataset, prompt_strategy, model, tokenizer, end_prompt=None):\n","    predictions = {\n","        'TP' : 0, # Sensitive\n","        'FP' : 0, # Non-sensitive document classified as sensitive\n","        'TN' : 0, # Non-sensitive\n","        'FN' : 0,\n","    }\n","    # Model output is not an expected sensitivity attribute\n","    further_processing_required = {}\n","    # All model output\n","    model_responses = {}\n","\n","    scikit_true = []\n","    scikit_pred = []\n","\n","    for sample in dataset.iterrows():\n","        sample_text = sample[1].text\n","        ground_truth = sample[1].sensitivity\n","\n","        # To replace with appropriate pre-processing\n","        if len(sample_text) > 12000:\n","            continue\n","        \n","        classification = prompt_to_reply(sample_text, prompt_strategy, model, tokenizer, end_prompt)\n","        model_responses[sample[1].doc_id] = classification\n","\n","        quadrant, pred = post_process_classification(classification, ground_truth)\n","        if pred == None:\n","            further_processing_required[sample[1].doc_id] = quadrant\n","            continue\n","\n","        predictions[quadrant] = predictions.get(quadrant) + 1\n","        scikit_true.append(ground_truth)\n","        scikit_pred.append(pred)\n","\n","        #clear_memory()\n","\n","    return predictions, further_processing_required, model_responses, scikit_true, scikit_pred\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def post_process_split_docs(mr, fpr, pre, df):\n","    clean_doc_id = {}\n","    ground_truths = []\n","    ite = -1\n","    for s in mr.keys(): #samp.doc_id():\n","        if s in fpr.keys():\n","            continue\n","\n","        if '_' in s:\n","            s = s[:s.find('_')]\n","\n","        val = clean_doc_id.get(s, -1)\n","        ite += 1\n","\n","        if val == -1:\n","            clean_doc_id[s] = pre[ite]\n","            ground_truths.append((df[df.doc_id == s].sensitivity).iloc[0])\n","            continue\n","\n","        if (val == pre[ite] or val == 1):\n","            continue\n","        \n","        clean_doc_id[s] = pre[ite]\n","\n","    values_array = np.array(list(clean_doc_id.values()))\n","    #print(ground_truths)\n","    #print(values_array)\n","    return values_array, ground_truths\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Evaluation summary"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def basic_metrics(true_labels, predictions):\n","    acc = accuracy_score(true_labels, predictions)\n","    bac = balanced_accuracy_score(true_labels, predictions)\n","    f1 = f1_score(true_labels, predictions, average=None)\n","\n","\n","def evaluation_summary(description, true_labels, predictions):\n","    target_classes = ['Non-sensitive (0)', 'Sensitive (1)']\n","    print(\"Evaluation for: \" + description)\n","    print(classification_report(true_labels, predictions, digits=3, zero_division=0, target_names=target_classes))\n","    print('\\n\\nConfusion matrix:')\n","    confusionMatrix = confusion_matrix(true_labels, predictions)\n","    disp = ConfusionMatrixDisplay(confusion_matrix=confusionMatrix, display_labels=target_classes) \n","    disp.plot()\n","    plt.show()\n","\n","def get_metric_dict(method, labels, preds):\n","    acc = accuracy_score(labels, preds)\n","    bac = balanced_accuracy_score(labels, preds)\n","    f1 = f1_score(labels, preds, average='weighted')\n","    \n","    metric_dict = {'accuracy': acc, 'f1_score': f1, 'balanced accuracy': bac}\n","    print(metric_dict)\n","\n"," \n","#evaluation_summary(\"Llama-2\", scikit_true, scikit_pred)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Prompts"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Init\n","def base(document):\n","  return f\"\"\"[INST]\n","The Text is sensitive if it contains sensitive personal information.\n","Which one of the attributes “sensitive”, “non-sensitive” classifies the Text. \n","The Text is delimited by triple backticks. You must answer after 'Attribute:'[/INST]\n","\n","Text: ```{document}```.\n","\n","Attribute:\"\"\"\n","\n","def b1(document):\n","  return f\"You are identifying documents containing personal sensitive information. Is the given text classified as 'sensitive' or 'non-sensitive'? You must answer with a the class inside a Python list and provide no further explanation, for example ['non-sensitive']. \\nText: {document} \\n\"\n","\n","def b2(document):\n","  return f\"You are identifying documents containing personal sensitive information. Is the given text classified as 'sensitive' or 'non-sensitive'? You must answer with a the class inside a Python list and provide no further explanation, for example ['sensitive']. \\nText: {document} \\n\"\n","\n","def b3(document):\n","  return f\"You are identifying documents containing personal sensitive information. \\nText: {document} \\nWhich one of the attributes 'sensitive', 'non-sensitive' describes a given text? You must answer with a Python list containing the only appropriate attribute and provide no further explanation. \\n\"\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Main experiment - main call"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# Get dataset\n","# Preprocess\n","# Get model\n","# Get prompt\n","# Run evaluation\n","# Display evaluation metrics\n","\n","def main_experiment(portion, portion_value, proc_meth, m, v, prompt_strategy, end_prompt, tokenizer, model):\n","    sara_dataset = get_sara()\n","    sara_df = dataset_to_df(sara_dataset)\n","    if portion == 'n':\n","        samp = get_sample_n(sara_df, portion_value)\n","    else:\n","        samp = get_sample_frac(sara_df, portion_value)\n","\n","    if proc_meth == 'replies':\n","        proc_samp = proc1(samp)\n","    elif proc_meth == 'replies_nometa':\n","        proc_samp = proc3(samp)\n","    else:\n","        proc_samp = proc2(samp)\n","        \n","    #tokenizer, model = get_model_version(m, v)\n","    predictions, further_processing_required, model_responses, truths, preds = llm_experiment(proc_samp, prompt_strategy, model, tokenizer, end_prompt)\n","    new_preds, new_truths = post_process_split_docs(model_responses, further_processing_required, preds, sara_df)\n","    #evaluation_summary(\"Llama-2\", truths, preds)\n","    #get_metric_dict(\"Llama-2\", truths, preds)\n","    print(model_responses)\n","    #evaluation_summary(\"Llama-2\", new_truths, new_preds)\n","    #get_metric_dict(\"Llama-2\", new_truths, new_preds)\n","    return model_responses"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["CUDA extension not installed.\n","CUDA extension not installed.\n"]}],"source":["m = 'get_llama2'\n","v = 'TheBloke/Llama-2-13B-chat-GPTQ' #\"meta-llama/Llama-2-7b-chat-hf\"\n","tokenizer, model = get_model_version(m, v)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def llm_inference(document, prompt, model, tokenizer):\n","    messages = [\n","    {\"role\": \"user\", \"content\": prompt(document)},\n","    ]\n","    encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n","    device = 'cuda'\n","    model_inputs = encodeds.to(device)\n","    arr_like = torch.ones_like(model_inputs)\n","    attention_mask = arr_like.to(device)\n","    generated_ids = model.generate(inputs=model_inputs, attention_mask=attention_mask, max_new_tokens=30, do_sample=True)\n","    decoded = tokenizer.batch_decode(generated_ids)\n","    return decoded[0]"]},{"cell_type":"markdown","metadata":{},"source":["Code not present for the experiments with above chat template. However, we found:\n","* The 70B model seems to answer most sensibly with only lists but can only operate on smaller documents (length roughly 1000). Whereas, the 7B models seem to not follow the appropriate template pattern. We primarily worked with the 13B 8 bit model.\n","* The chat template adds instruction tags. Attempted giving assistant role with content document, with prompt surrounding it from user role as instruction commands. Using a single user role with entire prompt in content seems to give the most sensible output.\n","* We requested a Python list output, following the literature.\n","* We also found some samples are empty strings from inspecting the models output when it requested for the text. We add a statement to exclude small strings from the sample during preprocessing."]},{"cell_type":"markdown","metadata":{},"source":["Further chat template experiments:"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["def b1(document):\n","  return f\"You are identifying documents containing personal sensitive information. Is the given text classified as 'sensitive' or 'non-sensitive'? \\nText: {document} \""]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["def llm_inference(document, prompt, model, tokenizer):\n","    messages = [\n","    {\"role\": \"system\", \"content\": \"You must answer with a Python list containing the only appropriate class and provide no further explanation.\"},\n","    {\"role\": \"user\", \"content\": prompt(document)},\n","    #{\"role\": \"assistant\", \"content\": \"\\n[\"}\n","    ]\n","    encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n","    device = 'cuda'\n","    model_inputs = encodeds.to(device)\n","    arr_like = torch.ones_like(model_inputs)\n","    attention_mask = arr_like.to(device)\n","    generated_ids = model.generate(inputs=model_inputs, attention_mask=attention_mask, max_new_tokens=30, do_sample=True)\n","    decoded = tokenizer.batch_decode(generated_ids)\n","    #print(decoded)\n","    return decoded[0]"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["#mr = main_experiment('n', 30, 'replies_nometa', 'get_llama2', \"meta-llama/Llama-2-7b-chat-hf\", b1, '[/INST]', tokenizer, model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Mistral"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.62s/it]\n"]}],"source":["def llm_inference(document, prompt, model):\n","  messages = [\n","    {\"role\": \"user\", \"content\": get_prompt_template(document, prompt)}\n","  ]\n","  encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n","  inputs = encodeds.to('cuda')\n","  ### Array like\n","  arr_like = torch.ones_like(inputs)\n","  attention_mask = arr_like.to('cuda')\n","  generated_ids = model.generate(inputs=inputs, attention_mask=attention_mask, max_new_tokens=150, do_sample=True)\n","  decoded = tokenizer.batch_decode(generated_ids)\n","  return decoded[0]\n","\n","device = \"cuda\"\n","model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", cache_dir=my_cache)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}
