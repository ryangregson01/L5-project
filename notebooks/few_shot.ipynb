{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n"]}],"source":["print(1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import ir_datasets\n","import email\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n","import gc\n","from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n","from config import *\n","import re\n","import numpy as np"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Data set-up, pre-processing and model set-up, evaluation metrics, experiment set-up"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import sys\n","sys.path.append(\"../scripts/\")"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from dataset import load_sara\n","from eval import jupyter_evaluation\n","from model import llm_experiment, post_process_split_docs\n","from models import get_model_version\n","from preprocess_sara import proccutit\n","\n","\n","#s = load_sara()\n","#proc = proccutit(s)\n","#tokenizer, model = get_model_version('get_mistral', \"mistralai/Mistral-7B-Instruct-v0.2\", 'main', 'auto')\n","#jupyter_evaluation(labels, preds)\n","#llm_experiment(dataset, prompt_strategy, model, tokenizer, end_prompt=None):\n","#post_process_split_docs(mr, fpr, pre, df)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["#tokenizer, model = get_model_version('get_mistral', \"mistralai/Mistral-7B-Instruct-v0.2\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def get_sample_n(data, n):\n","    return data.sample(n=n, random_state=1)\n","\n","def get_sample_frac(data, frac):\n","    return data.sample(frac=frac, random_state=1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Main experiment - main call"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Get dataset\n","# Preprocess\n","# Get model\n","# Get prompt\n","# Run evaluation\n","# Display evaluation metrics\n","\n","def main_experiment(portion, portion_value, proc_meth, prompt_strategy, end_prompt, tokenizer, model):\n","    sara_df = load_sara()\n","    if portion == 'n':\n","        samp = get_sample_n(sara_df, portion_value)\n","    else:\n","        samp = get_sample_frac(sara_df, portion_value)\n","\n","    if proc_meth:\n","        proc_samp = proccutit(samp)\n","    else:\n","        proc_samp = samp\n","        \n","    #tokenizer, model = get_model_version(m, v)\n","    preds, truths, model_responses, further_processing_required = llm_experiment(proc_samp, prompt_strategy, model, tokenizer, end_prompt)\n","    new_preds, new_truths = post_process_split_docs(model_responses, further_processing_required, preds, sara_df)\n","    jupyter_evaluation(new_truths, new_preds)\n","    return model_responses"]},{"cell_type":"markdown","metadata":{},"source":["### Investigation"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["#mr = main_experiment('n', 10, True, b1, '\\n[/INST]', tokenizer, model)\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>doc_id</th>\n","      <th>text</th>\n","      <th>sensitivity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>114715</td>\n","      <td>Message-ID: &lt;26804150.1075842955435.JavaMail.e...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>229405</td>\n","      <td>Message-ID: &lt;23075367.1075853128311.JavaMail.e...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>232795</td>\n","      <td>Message-ID: &lt;27422646.1075853196172.JavaMail.e...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>62815</td>\n","      <td>Message-ID: &lt;4131316.1075840896739.JavaMail.ev...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>118871</td>\n","      <td>Message-ID: &lt;12747077.1075843316348.JavaMail.e...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   doc_id                                               text  sensitivity\n","0  114715  Message-ID: <26804150.1075842955435.JavaMail.e...            0\n","1  229405  Message-ID: <23075367.1075853128311.JavaMail.e...            0\n","2  232795  Message-ID: <27422646.1075853196172.JavaMail.e...            0\n","3   62815  Message-ID: <4131316.1075840896739.JavaMail.ev...            0\n","4  118871  Message-ID: <12747077.1075843316348.JavaMail.e...            0"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["s = load_sara()\n","proc = proccutit(s)\n","tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", use_fast=True)\n","s.head()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (1415 > 1024). Running this sequence through the model will result in indexing errors\n"]}],"source":["gpt2_encdoed = {}\n","for samp in s.iterrows():\n","    idd = samp[1].doc_id\n","    text = samp[1].text\n","    encoded_text = tokenizer(text, return_tensors=\"pt\")\n","    vector_rep = encoded_text.input_ids[0]\n","    gpt2_encdoed[idd] = vector_rep\n","    #break"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["#gpt2_encdoed"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["array([[0.        , 0.        ],\n","       [0.57735027, 0.81649658]])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","X = [[0, 0, 0], [1, 1, 1]]\n","Y = [[1, 0, 0], [1, 1, 0]]\n","cosine_similarity(X, Y)"]},{"cell_type":"markdown","metadata":{},"source":["### Single sim"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["dam = list(gpt2_encdoed.values())"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def pad_token_sequences(seq1, seq2, pad_token_id=0):\n","    \"\"\"\n","    Pad the shorter sequence with the pad_token_id to match the length of the longer sequence.\n","    \"\"\"\n","    max_len = max(len(seq1), len(seq2))\n","    seq1_padded = seq1 + [pad_token_id] * (max_len - len(seq1))\n","    seq2_padded = seq2 + [pad_token_id] * (max_len - len(seq2))\n","    return seq1_padded, seq2_padded\n","\n","\n","bp = 0\n","max_sims = []\n","max_sims_is = []\n","for embed in dam:\n","    #print(embed)\n","    sims = []\n","    max_sim = 0\n","    max_sim_i = 0\n","    token_sequence_1 = embed\n","    token_sequence_1 = token_sequence_1.tolist()\n","    for i, seq in enumerate(dam):\n","        if embed is seq:\n","            continue\n","\n","        seq = seq.tolist()\n","        seq1_padded, seq_padded = pad_token_sequences(token_sequence_1, seq)\n","        # Now seq1_padded and seq_padded are ready for comparison\n","        c = cosine_similarity([seq1_padded], [seq_padded])\n","        sims.append(c)\n","        if max_sim == 0:\n","            max_sim = c\n","            max_sim_i = i\n","        elif c[0][0] > max_sim:\n","            max_sim = c[0][0]\n","            max_sim_i = i\n","\n","    if len(sims) != 1701:\n","        print(len(sims))\n","\n","    max_sims.append(max_sim)\n","    max_sims_is.append(max_sim_i)\n","\n","    bp += 1\n","    if bp == 20:\n","        break"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["20"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["bp"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.2712543406017331, 0.36048117663394597, 0.44465392891861005, 0.3267176534321523, 0.25942995930519586, 0.4506272135577234, 0.3092553486843901, 0.3461567001889617, 0.3502639591342497, 0.29175151416550393, 0.2613340103901019, 0.26378104924836476, 0.34748413059678585, 0.4561492149342725, 0.2335948570656921, 0.3550240281042519, 0.42497858684934375, 0.38604951812410404, 0.2798481761775982, 0.3043582683373962]\n","[308, 536, 780, 978, 337, 807, 1589, 1692, 881, 394, 262, 826, 1157, 1484, 141, 748, 463, 1511, 642, 216]\n"]}],"source":["print(max_sims)\n","print(max_sims_is)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["s_20 = s.head(20)\n","#s_20"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["sim_20 = s.loc[max_sims_is]"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["DOCUMENT 0\n","0\n","0\n","DOCUMENT 1\n","0\n","0\n","DOCUMENT 2\n","0\n","0\n","DOCUMENT 3\n","0\n","1\n","DOCUMENT 4\n","0\n","0\n","DOCUMENT 5\n","0\n","0\n"]}],"source":["for i in range(20):\n","    print('DOCUMENT', i)\n","    orig = s_20.loc[i]\n","    text_orig = orig.text\n","    sens_orig = orig.sensitivity\n","    sim_i = max_sims_is[i]\n","    sim = sim_20.loc[sim_i]\n","    text_sim = sim.text\n","    sens_sim = sim.sensitivity\n","\n","    #print('\\nTEXT ORIGINAL\\n')\n","    print(sens_orig)\n","    #print(text_orig)\n","    #print('\\nTEXT SIMILAR\\n')\n","    print(sens_sim)\n","    #print(text_sim)\n","\n","    if i == 5:\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Both examples"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def pad_token_sequences(seq1, seq2, pad_token_id=0):\n","    \"\"\"\n","    Pad the shorter sequence with the pad_token_id to match the length of the longer sequence.\n","    \"\"\"\n","    max_len = max(len(seq1), len(seq2))\n","    seq1_padded = seq1 + [pad_token_id] * (max_len - len(seq1))\n","    seq2_padded = seq2 + [pad_token_id] * (max_len - len(seq2))\n","    return seq1_padded, seq2_padded\n","\n","\n","bp = 0\n","max_sims = []\n","max_sims_is = []\n","for k, embed in gpt2_encdoed.items():\n","    #print(embed)\n","    sims = []\n","    sens_max_sim = 0\n","    sens_max_sim_i = 0\n","    non_sens_max_sim = 0\n","    non_sens_max_sim_i = 0\n","    token_sequence_1 = embed\n","    token_sequence_1 = token_sequence_1.tolist()\n","    for i, k in enumerate(gpt2_encdoed.keys()):\n","        #print(k)\n","        #print(i)\n","        seq = gpt2_encdoed[k]\n","        if embed is seq:\n","            continue\n","\n","        seq = seq.tolist()\n","        seq1_padded, seq_padded = pad_token_sequences(token_sequence_1, seq)\n","        # Now seq1_padded and seq_padded are ready for comparison\n","        c = cosine_similarity([seq1_padded], [seq_padded])\n","        sims.append(c)\n","\n","        seq_label = s[s['doc_id']==k].sensitivity.iloc[0]\n","        if seq_label == 0:\n","            if non_sens_max_sim == 0:\n","                non_sens_max_sim = c\n","                non_sens_max_sim_i = i\n","            elif c[0][0] > non_sens_max_sim:\n","                non_sens_max_sim = c[0][0]\n","                non_sens_max_sim_i = i\n","        elif seq_label == 1:\n","            if sens_max_sim == 0:    \n","                sens_max_sim = c\n","                sens_max_sim_i = i\n","            elif c[0][0] > sens_max_sim:\n","                sens_max_sim = c[0][0]\n","                sens_max_sim_i = i\n","\n","    if len(sims) != 1701:\n","        print(len(sims))\n","\n","    max_sims.append((non_sens_max_sim, sens_max_sim))\n","    max_sims_is.append((non_sens_max_sim_i, sens_max_sim_i))\n","\n","    bp += 1\n","    if bp == 5:\n","        break"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[(0.2712543406017331, 0.24425211507299968), (0.36048117663394597, 0.31092243457914404), (0.44465392891861005, 0.37165092149615914), (0.2994349630216824, 0.3267176534321523), (0.25942995930519586, 0.23238264877620254)]\n"]}],"source":["print(max_sims)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[(308, 1167), (536, 1470), (780, 933), (131, 978), (337, 857)]\n"]}],"source":["print(max_sims_is)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["173793 54591\n","       doc_id                                               text  sensitivity\n","556  173793_0  I think this is a great initiative, but it is ...            0\n","557  173793_1  Jeff:  I have followed up with other DOE polit...            0\n","558  173793_2  Subject: Mtg w/Bob Gee\\nCynthia and I met with...            0\n"]}],"source":["for i in range(5):\n","    orig = s.loc[i]\n","    text_orig = orig.text\n","    sens_orig = orig.sensitivity\n","\n","    non_sens_sim_i = max_sims_is[i][0]\n","    non_sens_sim = s.loc[non_sens_sim_i]\n","    #non_sens_text_sim = non_sens_sim.text\n","    #non_sens_sens_sim = non_sens_sim.sensitivity\n","    sens_sim_i = max_sims_is[i][1]\n","    sens_sim = s.loc[sens_sim_i]\n","    #sens_text_sim = sens_sim.text\n","    #sens_sens_sim = sens_sim.sensitivity\n","\n","    non_sens_id_sim = non_sens_sim.doc_id\n","    sens_id_sim = sens_sim.doc_id\n","\n","    print(non_sens_id_sim, sens_id_sim)\n","\n","    filtered_df2 = proc[proc['doc_id'].apply(lambda x: (x.startswith(non_sens_id_sim)))]\n","    print(filtered_df2)\n","    few_prompt = filtered_df2.iloc[0].text\n","    break\n","\n","\n","    if i == 5:\n","        break"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["#few_prompt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}
