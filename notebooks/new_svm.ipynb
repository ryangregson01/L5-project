{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n"]}],"source":["print(1)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### ML methods"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.dummy import DummyClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split\n","\n","import sys\n","sys.path.append(\"../scripts/\")\n","from dataset import load_sara\n","from preprocess_sara import full_preproc"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def ml_tech(train ,test):\n","    vectorizer = CountVectorizer()\n","    vectorizer.fit(train.text.tolist())\n","    train_features = vectorizer.transform(train.text.tolist())\n","    test_features = vectorizer.transform(test.text.tolist())\n","    train_labels = train.sensitivity\n","    test_labels = test.sensitivity\n","\n","    dummy_mf = DummyClassifier(strategy=\"most_frequent\")\n","    dummy_mf.fit(train_features, train_labels)\n","    mf_preds = dummy_mf.predict(test_features)\n","    #evaluation_summary(\"Dummy MF\", test_labels, mf_preds)\n","\n","    dummy_rand = DummyClassifier(strategy=\"stratified\")\n","    dummy_rand.fit(train_features, train_labels)\n","    rand_preds = dummy_rand.predict(test_features)\n","    #evaluation_summary(\"Dummy Random stratified sampling\", test_labels, rand_preds)\n","\n","\n","    tfidf_vectoizer = TfidfVectorizer()\n","    train_tfidf = tfidf_vectoizer.fit_transform(train.text.tolist())\n","    test_tfidf = tfidf_vectoizer.transform(test.text.tolist())\n","\n","    lr = LogisticRegression(max_iter=500)\n","    lr.fit(train_tfidf, train.sensitivity)\n","    lr_preds = lr.predict(test_tfidf)\n","    #evaluation_summary(\"LR\", test.sensitivity, lr_preds)\n","\n","    svm_model = SVC()\n","    svm_model.fit(train_tfidf, train.sensitivity)\n","    svm_preds = svm_model.predict(test_tfidf)\n","    #evaluation_summary(\"SVM\", test.sensitivity, svm_preds)\n","\n","    return mf_preds, rand_preds, lr_preds, svm_preds"]},{"cell_type":"markdown","metadata":{},"source":["## MAIN"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import ir_datasets\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, fbeta_score\n","from config import *\n","import re\n","import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.dummy import DummyClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def get_sara():\n","    return ir_datasets.load('sara')\n","\n","def dataset_to_df(sara_dataset):\n","    doc_ids = []\n","    doc_text = []\n","    doc_sens = []\n","    for doc in sara_dataset.docs_iter():\n","        doc_ids.append(doc.doc_id)\n","        doc_text.append(doc.text)\n","        doc_sens.append(doc.sensitivity)\n","\n","    sara_dict = {'doc_id':doc_ids, 'text':doc_text, 'sensitivity':doc_sens}\n","    df = pd.DataFrame.from_dict(sara_dict)\n","    return df\n","\n","def get_sample_n(data, n):\n","    return data.sample(n=n, random_state=1)\n","\n","def get_sample_frac(data, frac):\n","    return data.sample(frac=frac, random_state=1)\n","\n","\n","def evaluation_summary(description, true_labels, predictions):\n","    target_classes = ['Non-sensitive (0)', 'Sensitive (1)']\n","    print(\"Evaluation for: \" + description)\n","    print(classification_report(true_labels, predictions, digits=3, zero_division=0, target_names=target_classes))\n","    print('\\n\\nConfusion matrix:')\n","    confusionMatrix = confusion_matrix(true_labels, predictions)\n","    disp = ConfusionMatrixDisplay(confusion_matrix=confusionMatrix, display_labels=target_classes) \n","    disp.plot()\n","    plt.show()\n","\n","def tpr(labels, preds):\n","    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n","    tpr = tp / (tp+fn)\n","    return tpr\n","\n","def tnr(labels, preds):\n","    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n","    tnr = tn / (tn+fp)\n","    return tnr\n","\n","#def auroc(group):\n","#    return roc_auc_score(group['ground_truth'], group['prediction'])\n","\n","def get_metric_dict(method, labels, preds):\n","    acc = accuracy_score(labels, preds)\n","    bac = balanced_accuracy_score(labels, preds)\n","    f1 = f1_score(labels, preds, average='binary')\n","    prec = precision_score(labels, preds, average='binary')\n","    rec = recall_score(labels, preds, average='binary')\n","    f2 = fbeta_score(labels, preds, average='binary', beta=2)\n","    tpr_score = tpr(labels, preds)\n","    tnr_score = tnr(labels, preds)\n","    metric_dict = {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1_score': f1, 'bal accuracy': bac, 'f2_score': f2, 'tpr':tpr_score, 'tnr':tnr_score}\n","    return metric_dict\n","    metrics_data[method] = metric_dict"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["import email\n","import gensim\n","\n","def no_reply_proc(s, tokenizer='', c_size=2048):\n","    def preprocess(e):\n","        message = email.message_from_string(e)\n","        clean = message.get_payload()\n","        clean = re.sub('\\S*@\\S*\\s?', '', clean)\n","        clean = re.sub('\\s+', ' ', clean)\n","        clean = re.sub(\"\\'\", \"\", clean)\n","        clean = gensim.utils.simple_preprocess(str(clean), deacc=True, min_len=1, max_len=100) \n","        return clean\n","\n","    def remove_doubles(df):\n","        already_exists = []\n","        unique_df = []\n","        for i, s in enumerate(df.iterrows()):\n","            idd = s[1].doc_id\n","            text = s[1].text\n","            sensitivity = s[1].sensitivity\n","            if text in already_exists:\n","                continue\n","            already_exists.append(text)\n","            unique_df.append({'doc_id': idd, 'text':text, 'sensitivity':sensitivity})    \n","        return pd.DataFrame.from_dict(unique_df)\n","\n","    def main(s):\n","        processed_emails = [preprocess(a) for a in s.text]\n","        ids = s.doc_id.tolist()\n","        sens = s.sensitivity.tolist()\n","        texts = []\n","        for i, text in enumerate(s.text):\n","            new_email = ' '.join(processed_emails[i])\n","            texts.append(new_email)\n","\n","        new_dict = {'doc_id': ids, 'text': texts, 'sensitivity':sens}\n","        preproc_df = pd.DataFrame.from_dict(new_dict)\n","        preproc_df = remove_doubles(preproc_df)\n","        return preproc_df\n","\n","    return main(s)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'accuracy': 0.8833333333333333, 'precision': 0.6923076923076923, 'recall': 0.05921052631578947, 'f1_score': 0.10909090909090909, 'bal accuracy': 0.5278002090062701, 'f2_score': 0.07246376811594203, 'tpr': 0.05921052631578947, 'tnr': 0.9963898916967509}\n"]}],"source":["def main_experiment():\n","    sara_dataset = get_sara()\n","    sara_df = dataset_to_df(sara_dataset)\n","    sara_df = no_reply_proc(sara_df)\n","    train = sara_df\n","    svmatch(train)\n","\n","main_experiment()"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'accuracy': 0.8785714285714286, 'precision': 1.0, 'recall': 0.012903225806451613, 'f1_score': 0.025477707006369425, 'bal accuracy': 0.5064516129032258, 'f2_score': 0.01607717041800643, 'tpr': 0.012903225806451613, 'tnr': 1.0}\n"]}],"source":["def svmatch(data):\n","    #print(data.head())\n","    X = data.text.to_numpy()\n","    y = data.sensitivity.to_numpy()\n","\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8)\n","\n","\n","    tfidf_vectoizer = TfidfVectorizer()\n","    train_tfidf = tfidf_vectoizer.fit_transform(X_train)\n","    test_tfidf = tfidf_vectoizer.transform(X_test)\n","\n","    svm_model = SVC()\n","    svm_model.fit(train_tfidf, y_train)\n","    svm_preds = svm_model.predict(test_tfidf)\n","    #print(svm_preds)\n","    print(get_metric_dict('SVMMATCH', y_test, svm_preds))\n","    \n","mr = main_experiment()"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"empty vocabulary; perhaps the documents only contain stop words","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[43], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(get_metric_dict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSVMMATCH\u001b[39m\u001b[38;5;124m'\u001b[39m, y_test, svm_preds))\n\u001b[1;32m     32\u001b[0m     evaluation_summary(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSVM\u001b[39m\u001b[38;5;124m'\u001b[39m, y_test, svm_preds)\n\u001b[0;32m---> 34\u001b[0m mr \u001b[38;5;241m=\u001b[39m \u001b[43mmain_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[25], line 6\u001b[0m, in \u001b[0;36mmain_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m sara_df \u001b[38;5;241m=\u001b[39m no_reply_proc(sara_df)\n\u001b[1;32m      5\u001b[0m train \u001b[38;5;241m=\u001b[39m sara_df\n\u001b[0;32m----> 6\u001b[0m \u001b[43msvmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[43], line 24\u001b[0m, in \u001b[0;36msvmatch\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     21\u001b[0m y_train \u001b[38;5;241m=\u001b[39m y_train[combined_indices]\n\u001b[1;32m     23\u001b[0m tfidf_vectoizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m---> 24\u001b[0m train_tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_vectoizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m test_tfidf \u001b[38;5;241m=\u001b[39m tfidf_vectoizer\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[1;32m     27\u001b[0m svm_model \u001b[38;5;241m=\u001b[39m SVC()\n","File \u001b[0;32m/scratch/2469038g/miniconda3/envs/ryanenv2/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:2139\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2134\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2135\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2136\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2137\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2138\u001b[0m )\n\u001b[0;32m-> 2139\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2142\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n","File \u001b[0;32m/scratch/2469038g/miniconda3/envs/ryanenv2/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/scratch/2469038g/miniconda3/envs/ryanenv2/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m             )\n\u001b[1;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/scratch/2469038g/miniconda3/envs/ryanenv2/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1295\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1297\u001b[0m         )\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n","\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"]}],"source":["def cat(text):\n","    l = text.split('\\r\\n\\r\\n')\n","    return l[1]\n","\n","def svmatch(data):\n","    np.random.seed(1)\n","    #print(data.head())\n","\n","    #data['text'] = data['text'].apply(cat)\n","    #data = data[data.text.str.len() < 11000]\n","    \n","    X = data.text.to_numpy()\n","    y = data.sensitivity.to_numpy()\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=2)\n","\n","    majority_class_index = np.where(y_train == 0)[0]\n","    minority_class_index = np.where(y_train == 1)[0]\n","    downsampled_majority_index = np.random.choice(majority_class_index, size=len(minority_class_index), replace=False)\n","    combined_indices = np.concatenate([downsampled_majority_index, minority_class_index])\n","    X_train = X_train[combined_indices]\n","    y_train = y_train[combined_indices]\n","\n","    tfidf_vectoizer = TfidfVectorizer()\n","    train_tfidf = tfidf_vectoizer.fit_transform(X_train)\n","    test_tfidf = tfidf_vectoizer.transform(X_test)\n","\n","    svm_model = SVC()\n","    svm_model.fit(train_tfidf, y_train)\n","    svm_preds = svm_model.predict(test_tfidf)\n","    #print(svm_preds)\n","    print(get_metric_dict('SVMMATCH', y_test, svm_preds))\n","    evaluation_summary('SVM', y_test, svm_preds)\n","    \n","mr = main_experiment()\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.random.seed(1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["def get_set():\n","    sara_dataset = get_sara()\n","    sara_df = dataset_to_df(sara_dataset)\n","    sara_df = no_reply_proc(sara_df)\n","    train = sara_df\n","    return train\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["data = get_set()\n","X = data.doc_id.to_numpy()\n","y = data.sensitivity.to_numpy()\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=1)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"data":{"text/plain":["array(['54587', '173893', '173960', '173215', '176509', '234551',\n","       '175198', '173152', '52993', '128209', '174244', '173842',\n","       '175706', '176539', '176727', '177826', '175380', '10425',\n","       '175839', '173154', '174112', '125520', '175143', '54542', '70706',\n","       '175841', '176689', '173169', '173139', '127099', '230395',\n","       '44727', '129665', '121903', '173163', '52167', '175197', '174320',\n","       '173143', '136409', '175609', '121219', '176927', '178056',\n","       '175619', '54583', '175846', '175336', '175675', '1825', '52555',\n","       '54526', '175281', '151121', '9210', '174341', '175306', '174361',\n","       '175671', '174388', '174302', '174374', '136198', '54619',\n","       '174512', '54607', '173920', '175542', '123650', '175788', '54657',\n","       '174242', '176661', '176695', '54676', '175689', '136392',\n","       '173538', '175475', '175314', '173354', '176751', '173804',\n","       '219530', '175617', '54588', '7932', '47174', '176685', '175816',\n","       '175353', '173279', '176624', '174407', '175610', '176514',\n","       '173997', '173191', '239316', '139495', '175201', '177853',\n","       '174393', '175169', '174152', '173160', '173313', '176802',\n","       '173973', '173332', '19961', '173246', '175728', '175809', '82355',\n","       '174185', '63097', '174417', '54575', '138959', '125762', '174410',\n","       '173134', '55210', '229847', '174220', '219187', '174188',\n","       '173162', '176730', '175810', '174344', '175319', '114087',\n","       '57039', '174054', '175695', '54670', '173818', '175389', '221411',\n","       '229317', '175240', '174249', '9205', '54601', '174342', '173996',\n","       '173209', '176508', '120556', '115323', '175202', '176668',\n","       '239242', '173239', '174497', '119212', '175307', '174194',\n","       '173899', '54527', '173549', '54680', '175248', '3111', '174429',\n","       '173768', '253185', '233684', '136398', '176585', '221313',\n","       '173229', '136413', '173925', '173335', '175144', '174397',\n","       '173365', '177829', '253094', '136082', '175204', '176687',\n","       '54609', '54658', '174442', '175241', '162684', '174021', '174243',\n","       '126473', '118675', '174123', '173771', '173224', '177833',\n","       '123495', '54639', '232406', '175502', '175246', '137905',\n","       '175445', '54557', '54637', '229024', '176658', '177824', '174465',\n","       '115761', '174396', '234655', '174373', '175428', '139361',\n","       '175142', '229816', '175715', '174206', '176698', '54562',\n","       '174516', '175158', '173848', '174345', '54666', '175968',\n","       '175089', '136389', '175729', '175456', '174354', '175176',\n","       '173894', '54580', '9275', '174233', '175752', '175700', '173865',\n","       '173188', '173236', '175704', '211401', '175139', '235559',\n","       '54569', '54608', '114844', '174392', '175797', '175662', '175324',\n","       '54606', '173242', '54565', '137618', '175309', '173151', '227795',\n","       '175835', '174174', '55209', '54628', '179001', '174357', '175372',\n","       '174298', '175355', '175842', '175570', '136555', '173898',\n","       '129475', '228995', '54681', '173905', '173221', '114987', '9177',\n","       '54665', '136388', '175676', '175977', '173190', '175218',\n","       '175138', '121758', '174177', '232926', '175391', '124533',\n","       '255366', '174395', '176527', '124868', '175430', '175679',\n","       '174454', '175703', '176186', '137641', '114503', '123775',\n","       '136192', '173146', '239959', '175795', '124277', '173414',\n","       '173581', '54555'], dtype=object)"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["X_train"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/plain":["314"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["len(X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}
