@article{otter2020survey,
  title={A survey of the usages of deep learning for natural language processing},
  author={Otter, Daniel W and Medina, Julian R and Kalita, Jugal K},
  journal={IEEE transactions on neural networks and learning systems},
  volume={32},
  number={2},
  pages={604--624},
  year={2020},
  publisher={IEEE}
}

@inproceedings{mcdonald2015using,
  title={Using part-of-speech n-grams for sensitive-text classification},
  author={McDonald, Graham and Macdonald, Craig and Ounis, Iadh},
  booktitle={Proceedings of the 2015 International conference on the theory of information retrieval},
  pages={381--384},
  year={2015}
}

@article{noh2021improved,
  title={Improved biomedical word embeddings in the transformer era},
  author={Noh, Jiho and Kavuluru, Ramakanth},
  journal={Journal of biomedical informatics},
  volume={120},
  pages={103867},
  year={2021},
  publisher={Elsevier}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{he2020deberta,
  title={Deberta: Decoding-enhanced bert with disentangled attention},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv preprint arXiv:2006.03654},
  year={2020}
}

@article{gambarelli2023your,
  title={Is Your Model Sensitive? SPEDAC: A New Resource for the Automatic Classification of Sensitive Personal Data},
  author={Gambarelli, Gaia and Gangemi, Aldo and Tripodi, Rocco},
  journal={IEEE Access},
  volume={11},
  pages={10864--10880},
  year={2023},
  publisher={IEEE}
}

@article{pereira2023here,
  title={Here's to the future: Conversational agents in higher education-a scoping review},
  author={Pereira, Daniela SM and Falc{\~a}o, Filipe and Costa, Lilian and Lunn, Brian S and P{\^e}go, Jos{\'e} Miguel and Costa, Patr{\'\i}cio},
  journal={International Journal of Educational Research},
  volume={122},
  pages={102233},
  year={2023},
  publisher={Elsevier}
}

@article{white2023prompt,
  title={A prompt pattern catalog to enhance prompt engineering with chatgpt},
  author={White, Jules and Fu, Quchen and Hays, Sam and Sandborn, Michael and Olea, Carlos and Gilbert, Henry and Elnashar, Ashraf and Spencer-Smith, Jesse and Schmidt, Douglas C},
  journal={arXiv preprint arXiv:2302.11382},
  year={2023}
}

@article{gao2020making,
  title={Making pre-trained language models better few-shot learners},
  author={Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  journal={arXiv preprint arXiv:2012.15723},
  year={2020}
}

@article{qin2023chatgpt,
  title={Is ChatGPT a general-purpose natural language processing task solver?},
  author={Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},
  journal={arXiv preprint arXiv:2302.06476},
  year={2023}
}

@article{amin2023will,
  title={Will affective computing emerge from foundation models and general ai? A first evaluation on chatgpt},
  author={Amin, Mostafa M and Cambria, Erik and Schuller, Bj{\"o}rn W},
  journal={arXiv preprint arXiv:2303.03186},
  year={2023}
}

@article{yin2019benchmarking,
  title={Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach},
  author={Yin, Wenpeng and Hay, Jamaal and Roth, Dan},
  journal={arXiv preprint arXiv:1909.00161},
  year={2019}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{zhou2022large,
  title={Large language models are human-level prompt engineers},
  author={Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
  journal={arXiv preprint arXiv:2211.01910},
  year={2022}
}

@article{han2022ptr,
  title={Ptr: Prompt tuning with rules for text classification},
  author={Han, Xu and Zhao, Weilin and Ding, Ning and Liu, Zhiyuan and Sun, Maosong},
  journal={AI Open},
  volume={3},
  pages={182--192},
  year={2022},
  publisher={Elsevier}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{fei2023reasoning,
  title={Reasoning Implicit Sentiment with Chain-of-Thought Prompting},
  author={Fei, Hao and Li, Bobo and Liu, Qian and Bing, Lidong and Li, Fei and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2305.11255},
  year={2023}
}

