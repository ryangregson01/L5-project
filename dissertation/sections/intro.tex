Large collections of digital documents contain sensitivities that must be managed and protected, such as sensitive personal information which can compromise an individual's safety. Archivists face the task of making these collections accessible safely \cite{gollins2014using}. Before these collections are made available, documents must be manually pruned by sensitivity review experts to ensure sensitive information is not released.

The increasing volume of digital documents makes conducting a fully manual sensitivity review less feasible. To alleviate reviewers, sensitivity classifiers \cite{mcdonald2014towards, mcdonald2017enhancing, baron2022providing} and review systems \cite{narvala2022sensitivity} to assist the sensitivity review process have been implemented and analysed. However, experts in the sensitivity review process still encounter challenges in navigating the extensive volume of digital content. Moreover, under the Freedom of Information Act 2000 (FOIA) \cite{FOIA:00}, individuals are entitled to submit a freedom of information (FOI) request to government departments, local authorities, and other publicly funded bodies, yet national statistics show a growing number of delays in responding to these requests within the expected time limit \cite{SG:23, ONS:23}. These delays damage relationships between public bodies and requesters, as limited access to information decreases the transparency of public bodies. The issues of navigating through potentially sensitive documents is likely to be exacerbated as digital archive collections expand, further challenging the feasibility of manual sensitivity review.

Therefore, technological advancements are necessary for protecting sensitive content and ensuring the sensitivity review process is more feasible than exhaustive manual (and technology-assisted) review. Despite this ideal reviewing tool, the importance of protecting sensitive information means all government documents reviewed in the foreseeable future will have some level of manual review \cite{TNA:16}. This is reasonable as there is uncertainty around classifiersâ€™ capabilities. Identifying more reliable classifiers is an ongoing task, and studies show more accurate and confident classifiers benefit reviewers; increasing their reviewing speed when used within a review system \cite{mcdonald2020accuracy}.

Email collections can be enormous as emails are among the fastest-created types of digital documents due to regular and daily communication sent via this channel. Emails can be valuable to archives because they offer detailed insights into the development of ideas and help us understand communication, which enriches other research. Currently, many archives do not collect emails due to privacy concerns and the volume of documents in an email collection \cite{TFTAEA:18}. Companies may also collect emails to monitor breaches of company secrets \cite{iqbal2021search}. Therefore, tools must be produced to identify sensitive personal information and remove documents to ensure individuals' privacy and security are maintained when exploring email collections. Aligning with current developments, we explore a novel approach using generative AI to perform sensitivity reviews to identify sensitive personal information, aiming to protect sensitive documents.

%\subsection{Motivation}
With the increased use of generative AI dominating natural language understanding tasks in other domains \cite{qiu2020pre, adiwardana2020towards, roller2020recipes, openai2023gpt}, we aim to investigate the effectiveness of large language models (LLMs) in understanding and generating classifications and explanations concerning sensitive personal information within potentially sensitive email documents. In 2023, Baron et al. proposed the first use of a LLM to identify and explain sensitive information under FOIA Exemption 5 (the deliberative process privilege) \cite{baron2023using}. Their approach uses the pre-trained LLM, ChatGPT-3.5 \cite{brown2020language}, and evaluates how prompt variations influence the model's responses for zero-shot learning. Our work extends the exploration of LLMs in sensitivity detection, specifically focusing on identifying sensitive personal information (exempt under FOIA Section 40) with generative LLMs, where to the best of our knowledge no research effort has been made. Our research aims to contribute to the ongoing advancement of these models, enhancing their capabilities in detecting sensitive personal information and facilitating automated sensitivity review.

There are multiple strategies to enhance the performance of LLMs. One method is fine-tuning, used to also enhance other neural models; however, this is very expensive for LLMs \cite{naveed2023comprehensive}. Furthermore, training datasets labelled with sensitivity annotations can be small or non-existent for some sensitive domains, resulting in weaker model tuning. Another recent strategy to improve the performance of LLMs is prompt engineering \cite{liu2023pre}. Prompt engineering aims to give pre-trained LLMs awareness of contextual nuances, emphasising focus on specific characteristics of the data. Successful research into zero-shot prompt engineering \cite{wei2023zero, kojima2022large} shows the potential for exploring this less resource-intensive prompting strategy when using LLMs to identify sensitive personal information.

%\subsection{Research Contributions}
This paper aims to explore if generative LLMs can improve on current methods for identifying sensitive personal information, and the effect of prompt engineering for sensitivity classification with LLMs. By leveraging LLMs and prompt engineering techniques we aim to show the possibility of automatic classification of sensitive information. With feasible (accurate and efficient) approaches, the scalability of handling and protecting sensitive personal information can be improved. In this paper we present a novel use of pre-trained LLMs with prompt engineering techniques to identify sensitive personal information.

We investigate prompts that add contextual information about the dataset and sensitive personal information, few-shot learning and chain-of-thought prompt engineering techniques with well-regarded open-source decoder-only LLMs Mistral \cite{jiang2023mistral}, Mixtral \cite{jiang2024mixtral}, and Llama 2 \cite{touvron2023llama}. Our experiments show that with the inclusion of our prompt engineering techniques, these generative LLMs become significantly more effective at classifying sensitive personal information; where Mistral performs best with prompt $S\_EC+PS+FS$ (context of possible sensitive email categories, description about personal sensitivity, and few-shot). Mistral with this prompt makes statistically significant improvements compared to our $Base$ prompting strategy in sensitivity classification (McNemar's test $p<0.05$), achieving a 14.58\% balanced accuracy and 26.07\% F\textsubscript{2}-score increase.

The remainder of this paper is structured as follows. In Section 2 we present work relating to sensitivity classification and LLMs with prompt engineering. In Section 3, we motivate our dataset, model choice and prompting strategies, and in Section 4, we discuss our experimental setup. In Section 5 we present our results, then provide some further analysis in Section 6. Finally, we make our conclusions in Section 7.
