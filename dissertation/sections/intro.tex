Freedom of information laws are used around the world to give individuals the legal right to access information held by public authorities. In the UK, the Freedom of Information Act 2000 (FOIA) \cite{FOIA:00} entitles individuals to submit a freedom of information (FOI) request to government departments, local authorities, universities, and other publicly funded bodies. However, some information is exempt from release \cite{FOIA:00}; containing sensitivities that should not be accessible by the public, such as personal information. Before any requested information is available, documents must be pruned to ensure sensitive information is not released.

The increasing volume of digital documents is making it less feasible to conduct a fully manual sensitivity review. To alleviate reviewers, sensitivity classifiers \cite{mcdonald2014towards, mcdonald2017enhancing, baron2022providing} and review systems \cite{narvala2022sensitivity} to assist the sensitivity review process have been implemented and analysed. However, experts in the sensitivity review process still encounter challenges in navigating the extensive volume of digital content. National statistics show a growing number of delays in FOI request responses \cite{SG:23, ONS:23}. These delays damage relationships between public bodies and requesters, as limited access to information decreases the transparency of public bodies. The issues of navigating through potentially sensitive documents is likely to be exacerbated as digital archive collections expand, further challenging the feasibility of manual sensitivity review.

Therefore, technological advancements are necessary for protecting sensitive content and ensuring the sensitivity review process is more feasible than exhaustive manual (and technology-assisted) review. Despite this ideal reviewing tool, due to the importance of protecting sensitive information, all government documents reviewed in the foreseeable future will have some level of manual review \cite{TNA:16}. This is reasonable as there is uncertainty around classifiersâ€™ capabilities. Identifying more reliable classifiers is an ongoing task, and studies show more accurate and confident classifiers benefit reviewers; increasing their reviewing speed when used within a review system \cite{mcdonald2020accuracy}.

Emails are among the fastest-created types of digital documents, and regular, daily, communication via this channel results in enormous email collections. Email documents are valuable to archives as they offer detailed insights into the development of ideas and help us understand communication, which enriches other research [CITE]. Currently, many archives do not collect emails due to privacy concerns and the volume of documents in an email collection \cite{TFTAEA:18}. Companies may also collect emails to monitor breaches of company secrets. Tools must identify sensitive personal information and remove the email document to ensure individuals' privacy and security are maintained when exploring email collections. Therefore, we explore a novel approach with generative AI to perform sensitivity review to identify sensitive personal information, protecting sensitive documents.

%\subsection{Motivation}
With the increased use of generative AI dominating natural language understanding tasks in other domains \cite{qiu2020pre, adiwardana2020towards, roller2020recipes, openai2023gpt}, we aim to investigate the effectiveness of large language models (LLMs) in understanding and generating classifications and explanations concerning sensitive personal information within potentially sensitive documents. In 2023, Baron et al. proposed the first use of a LLM to identify and explain sensitive information under FOIA Exemption 5 (the deliberative process privilege) \cite{baron2023using}. Their approach uses the pre-trained LLM, ChatGPT-3.5 \cite{brown2020language}, and evaluates how prompt variations influence the model's responses for zero-shot learning. Our work aims extends the exploration of LLMs in sensitivity detection, specifically focusing on identifying sensitive personal information (exempt under FOIA Section 40) with generative LLMs, where to the best of our knowledge no research effort has been made. Our research aims to contribute to the ongoing advancement of these models, enhancing their capabilities in detecting sensitive personal information and facilitating automated sensitivity review.

There are multiple strategies to enhance the performance of LLMs. One method is finetuning, used to also enhance other neural models; however, this is very expensive for LLMs \cite{naveed2023comprehensive}. Furthermore, training datasets labelled with sensitivity annotations can be small or non-existent for some sensitive domains, resulting in weaker model tuning. Another recent strategy to improve the performance of LLMs is prompt engineering \cite{liu2023pre}. Prompt engineering aims to give pre-trained LLMs awareness of contextual nuances, emphasising focus on specific characteristics of the data. Successful research into zero-shot prompt engineering \cite{wei2023zero, kojima2022large} shows the potential for exploring this less resource-intensive prompting strategy when using LLMs to identify sensitive personal information.

%\subsection{Research Contributions}
This paper aims to explore if generative LLMs can improve on current methods for identifying sensitive personal information, and the effect of prompt engineering for sensitivity classification with LLMs. By leveraging LLMs and prompt engineering techniques we aim to show the possibility of automatic classification of sensitive information. With feasible (accurate and efficient) approaches, the scalability of handling and protecting sensitive personal information can be improved. In this paper we present a novel use of pre-trained LLMs with prompt engineering techniques to identify sensitive personal information.

We investigate prompts that add contextual information about the dataset and sensitive personal information, few-shot learning and chain-of-thought prompt engineering techniques with well-regarded open-source decoder-only LLMs Mistral, Mixtral, and Llama 2. Our experiments show that with the inclusion of our prompt engineering techniques, these generative LLMs become significantly more effective at classifying sensitive personal information; where Mistral with our top performing prompt (prompt) has a X\% balanced accuracy and Y\% $F_2$-score increase compared to our base prompt.

