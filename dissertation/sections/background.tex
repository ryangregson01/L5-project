\label{sec:background}
We discuss related work of sensitivity classifiers, which identify sensitivities in potentially sensitive digital documents. These classifiers have traditionally used machine learning techniques such as support vector machines (SVMs) and involved feature engineering. Technology-assisted review (TAR) systems have utilised these classifiers to enhance the manual sensitivity review process. We then explore large language models (LLMs): a deep learning technique used for natural language tasks. LLMs have advanced significantly, with research shifting to explore these models, where studies show performance near human-level reasoning and strong contextual understanding \cite{openai2023gpt}. This motivates our project aims as LLMs have not been significantly explored to detect sensitive content and this task requires nuanced understanding of documents. Finally, we review prompt engineering and how this technique is used to enhance LLMs understanding. We believe there are prompting techniques and patterns from the literature that are relevant to the task of automatic sensitivity classification.

\subsection{Sensitivity Detection}
\label{sec:background:sensitivity_detection}
In 2014, the first classifier for digital sensitivity review was introduced to classify government records as sensitive regarding FOIA exemptions, namely Section 27 (International Relations) and Section 40 (Personal Information) \cite{mcdonald2014towards}. Results from this study shows a text classification baseline could be improved with manually created features tailored for sensitivity detection. However, results also demonstrate that features such as country risk, relevant for international relations, negatively impacted classification of personal information sensitivities. This highlights the feasibility of sensitivity classifiers and reveals the requirement that features for different types of sensitivities must be considered separately. In our approach we use neural models; therefore, we do not have to manually create multiple feature sets for our sensitivities. Furthermore, recent trends have shown that deep learning models that work directly on the text tend to be more effective than traditional feature engineering approaches for most tasks \cite{otter2020survey}. This motivates our use of deep learning techniques for sensitivity classification.

Understanding the context of documents under review is important for correct sensitivity classification. McDonald et al. advanced their work of the first classifier for digital sensitivity review \cite{mcdonald2014towards}: proposing extensions to text classification with part-of-speech features \cite{mcdonald2015using}, as well as semantic (word embedding) features and additional term n-grams \cite{mcdonald2017enhancing} to identify international relations and personal information sensitivities. Using a classifier with semantic features made significantly more accurate predictions by identifying latent sensitive relations in documents \cite{mcdonald2017enhancing}. However, word embeddings only provide limited context about the overall sentence, whereas newer transformers facilitate contextual embeddings given the surrounding text \cite{noh2021improved}. One study using pre-trained extensions of BERT \cite{devlin2018bert} (RoBERTa \cite{liu2019roberta} and DeBERTa \cite{he2020deberta}) concludes these transformer methods perform better than traditional machine learning methods when classifying sensitive personal data \cite{gambarelli2023your}. These papers highlight a need for classifiers to be aware of the context of the document to capture subtle and latent sensitivities within sensitive documents. Therefore, we leverage these context-aware transformer-based language models in our sensitivity classification task. Additionally, context can be provided through prompting LLMs; therefore, we investigate how prompt engineering can provide necessary supplementary context for enhancing the identification of sensitivities.

Sensitivity review was once a fully manual approach; however, this has become infeasible as the number of digitally produced documents is ever increasing. Currently, sensitivity review systems with built-in sensitivity classifiers are used to assist expert reviewers, aiming to increase the throughput of reviewed documents. Analysis of these systems demonstrate reviewing accuracy and speeds increased given improved sensitivity classification accuracy \cite{mcdonald2020accuracy}. Therefore, improving sensitivity classification is important for both assisting manual review, and pushing the boundary for a classifier that does not require final human involvement. In addition to potentially improving classification, these generative models have been treated as conversational agents \cite{pereira2023here} and could also be explored as a personal assistant to expedite the review process. We aim to push the boundary; investigating if generative LLMs can classify sensitive documents. Recent research indicates that these models exhibit enhanced contextual understanding and reasoning capabilities \cite{adiwardana2020towards}. Our objective is to explore whether LLMs can be effectively applied to identify sensitive content in documents and remove human involvement.

Recent research by Baron et al. has explored using a pre-trained LLM without any finetuning to identify government records that are exempt under FOIA Exemption 5 \cite{baron2023using}. Baron et al. evaluated the zero-shot performance of the prominent language model ChatGPT-3.5 \cite{brown2020language} across various prompts. Results from this experiment show that ChatGPT (as a classifier) is worse than the supervised text classification methods explored in their previous work using the same documents \cite{baron2022providing}. However, ChatGPT’s generated text was deemed useful for reasoning about its classification choices. Baron et al. conclude this is the beginning of an investigation into generative AI to protect sensitive content \cite{baron2023using}. This work lays a foundation for classifying sensitive content with generative LLMs and motivates exploration of prompt engineering. This importantly motivates our plan to use pre-trained generative LLMs as it demonstrates the decision-making abilities of generative LLMs in protecting sensitive content. Baron et al. use OpenAI’s API for ChatGPT which is expensive. Therefore, our investigation will instead explore open-source LLMs (discussed further in Section \ref{sec:experiment:LLMs}) because they have no cost barrier. This means the models investigated in our research will be useful for individuals requiring extensive use of sensitivity classifiers; for example, archivists who aim to collect new digital documents such as emails without a large budget. We are also interested by the enhancements to model performance due to prompt engineering. However, Baron et al. do not consider notable prompt engineering techniques found in the literature to enhance model performance \cite{white2023prompt}. We explore the effect of more advanced prompting techniques for identifying sensitive content.

\subsection{Large Language Models (LLMs)}
Large language models (LLMs) have achieved excellent results in natural language processing tasks. These tasks can be split into categories such as general language understanding, question answering, sentiment analysis, named entity recognition, machine translation, and summarisation \cite{qiu2020pre}. Expensive strategies, such as standard fine-tuning have been used to enhance model accuracy. However, recent consideration of prompt engineering shows this promising new paradigm can enhance model performance given no (or little) prior knowledge of the task beforehand; allowing effective zero-shot (or few-shot) learning with the LLM \cite{liu2023pre}.

LLMs have been used successfully in document classification such as sentiment analysis tasks \cite{gao2020making, qin2023chatgpt, amin2023will} and text classification tasks [54, 73]. A study comparing ChatGPT to SOTA solutions shows ChatGPT outperforms the SOTA solution in a sentiment analysis task [3]. However, another thorough analysis of ChatGPT by Kocon et al. shows ChatGPT struggles most with emotional tasks [30]. Emotional tasks require a strong understanding of textual sentiment. Kocon et al. also show that for emotional tasks, such as identifying unhealthy conversations, ChatGPT still perceives sentiment more accurately than some human annotators. Hence, emotion classification is a difficult task; requiring pragmatic understanding of language that even SOTA models perform poorly at. Pragmatic tasks require the model to use additional knowledge that is not explicitly represented by distributional semantics \cite{kocon2023chatgpt}. As sensitivity classification requires understanding of the content in the document, this is a pragmatic task. Therefore, our approach employs prompting techniques to supplement the LLM in comprehending the question enquiring if sensitivity is present in the document, and for understanding the document to be analysed.

\subsubsection{Prompting LLMs}
Prompts are important in improving LLM responses. LLMs are effective with prompts as they resemble tasks that were solved during the original training process; for example, generating the next blanked word in the sentence \cite{liu2023pre}. In 2021, claims of shifting paradigms from ‘pretrain, then fine-tune’ to ‘pre-train, then prompt’ were stated \cite{liu2023pre}. Kocon et al state the “performance of modern language models, such as T5, GPT-3, and ChatGPT, heavily relies on the quality of task-specific prompts,” \cite{kocon2023chatgpt}. Basic prompting methods would provide a LLM with a question and the document text. We specialise prompts to assist the LLM in automatic sensitivity classification. Therefore, to produce high quality prompts for our task of sensitivity classification, we investigate different prompt engineering techniques to discover the most appropriate prompts.

Tuning-free prompting is a prompting method that does not change the parameters of the pre-trained LM; relying on the prompt to solely effect the possible answer from the model. Large closed-source models like ChatGPT \cite{brown2020language} are used this way, where the model is frozen and users can only prompt it. This approach is referred to as zero-shot learning and is less expensive than fine-tuning a LLM. However, with no fine-tuning, heavy prompt engineering is necessary as prompts are the only way to describe the task to the model \cite{liu2023pre}. This means this task description must be stated coherently. Therefore, we rigorously investigate which prompt statements assist LLMs when classifying sensitivity.

Another prompting method, fixed-LM prompt tuning, can be seen to be better than tuning-free prompting due to achieving higher accuracy on benchmark tasks \cite{liu2023pre}. Fixing the prompt and fine-tuning the LM is explored in the literature and results show this method is effective for text classification \cite{yin2019benchmarking}. However, this prompting method has the fine-tuning expense, and recent prompt engineering patterns \cite{white2023prompt} cannot be used as effectively as prompts are usually not human-interpretable \cite{liu2023pre}. Therefore, we focus instead on tuning-free prompting to see if recent prompt patterns can enhance LLMs when automatically classifying sensitive documents. Furthermore, small datasets can be more harmful when fine-tuning models \cite{zhao2023survey}, and most sensitive documents are not public unlike movie reviews for example. Thus, exploring tuning-free prompting evaluates if prompt engineering is effective for eliciting LLMs pre-trained knowledge to classify sensitive documents.

\subsection{Prompt Engineering}
\label{sec:background:prompt_engineering}
Various prompt engineering techniques have been explored in the literature, with a prompt pattern catalogue recently published (discussing seventeen prompt patterns) by White et al. \cite{white2023prompt}. This catalogue focuses on prompts for conversational LLMs, such as chatbot ChatGPT \cite{brown2020language}; however, many strategies can be used and adapted to our less interactive classification task. We believe the notable techniques relevant to the sensitivity detection task include: the cognitive verifier pattern, template pattern, and the context manager pattern.

Literature shows LLMs can reason better if a question is divided into additional smaller questions that help to answer the initial question \cite{zhou2022large}. The \textbf{Cognitive Verifier Pattern} is useful with ChatGPT, with the recommended prompt: “When I ask you a question, generate three additional questions that would help you give a more accurate answer. When I have answered the three questions, combine the answers to produce the final answers to my original question.” \cite{white2023prompt}. This is beneficial for our task as the LLM may ask specific questions about the document, clarifying weak understanding which can lead to a more accurate classification. However, this pattern requires user input for every document as the questions may be different for each document, making it less feasible for our investigation of a fully automated approach.

A similar strategy to the cognitive verifier pattern is \textbf{prompt composition} \cite{liu2023pre}. This method answers additional primer questions composed by us before answering the main question of classifying sensitivity. Instead of questions, we use sub-tasks for the LLM to acknowledge or reason about, which contribute to the main classification question \cite{han2022ptr}. Intermediate reasoning is used by the popular \textbf{Chain-of-Thought (CoT)} method \cite{wei2022chain, fei2023reasoning}. This method decomposes a problem into steps the model should take. It evokes ‘thought’ by asking the model to consider smaller separate questions at each step, and has been shown to improve LLMs' capability in complex reasoning tasks \cite{wei2022chain}. Sensitivity classification requires strong reasoning capabilities; therefore, we use this pattern to investigate if reasoning about sensitive personal information can improve the LLM's effectiveness compared to a baseline prompt.

\textbf{Answer engineering} is a strategy to constrain the response of the model \cite{liu2023pre}. In the literature, generative LLMs have used this technique when classifying text; for example, in sentiment classification tasks \cite{yin2019benchmarking, kocon2023chatgpt}. White et al. regard this technique as the \textbf{Template Pattern} \cite{white2023prompt}. This pattern is useful as it allows efficient processing of the generated text. A disadvantage of constraining model output is that other useful output the LLM may have generated is avoided. Baron et al. do not enforce a strict output pattern when using ChatGPT to detect sensitive content \cite{baron2023using}. Their study discusses that ChatGPT's explanations were its most interesting capability. However, our aim focuses on evaluating an automatic sensitivity classifier; therefore, we use answer engineering to control the output, retrieving the classification label for downstream tasks. Furthermore, although the output is constrained, this restriction only regards the first word generated. This means the model may continue to provide explanations that are useful, such as justifications for the classification which we can use in our manual analysis.

The \textbf{Context Manager Pattern} emphasises specific aspects of the task \cite{white2023prompt}. By emphasising information, this pattern assists the LLM by providing additional context about the document and the aim of the task. We believe this can better inform the LLM about what sensitive information should be identified, and using terms understood from pre-training can assist zero-shot classification. We know context is important for correct sensitivity classification, as from the literature sensitivity classifiers that utilised semantic features could identify latent sensitivities within sensitive documents \cite{mcdonald2017enhancing}, and transformer-based methods proved to be effective identifying sensitive personal data \cite{gambarelli2023your} (discussed in Section \ref{sec:background:sensitivity_detection}). Therefore, we aim to assess if the context manager prompt pattern can improve the effectiveness of LLMs for sensitivity classification by introducing context that directs the model to identify sensitivities we are interested in.

\textbf{Few-shot learning}
In the literature, a well-studied LM training strategy is few-shot learning. When prompting, this is also known as in-context learning as the model parameters are not actually updated; instead, the model is given additional in-context information to process \cite{gao2020making, brown2020language}. This prompting strategy uses annotated samples within the prompt before the document of interest. Is has been shown this improves the effectiveness of LLMs' and aligns generated text to expected output, which complements answer engineering for classification tasks. A concern is that LLMs have a limited context window; however, we choose models with an appropriate context window length for presenting example documents (discussed further in Section \ref{sec:experiment:LLMs}). Therefore, we explore if an in-context learning strategy improves sensitivity classification.

