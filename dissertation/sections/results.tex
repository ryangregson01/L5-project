\section{Results}

\subsection{RQ1. How effective are generative LLMs for identifying sensitive personal information within emails.}

We first analyse the effectiveness of different generative LLMs. Table \ref{tab:results} contains the results of Mistral, Mixtral, Llama 2, and Flan-T5 for SARA. All of these models have experienced instruction-finetuning, making them appropriate to use for classification and reasoning-based tasks. We observe that Mistral models outperform Llama-2 and Flan-T5, where Mistral obtains the highest BAC and F1 scores, 0.6790 and 0.3369 respectively. Mistral performance over Llama 2 is also in agreement with other tasks [CITE]. We conclude Mistral is (statistically) significantly better than Llama 2 for identifying sensitive personal information within our email collection using McNemar’s test (statistic value). Interestingly, the Mixtral model that is larger than Mistral does not outperform Mistral in sensitivity classification, where BAC and F1 scores for Mixtral’s best prompt CG+FS are 0.6738 and 0.3398 respectively, whereas scores for Mistral’s best performing prompt CG 0.6790 and 0.3369. We believe these differences are minor to the overall classification effectiveness. Both of these models are statistically significantly different (McNemar’s p-value of 0.0182), where they disagree about 277 documents: Mistral correctly classifying 160 documents (12 sensitive, 172 non-sensitive) and Mixtral correctly classifying 117 different documents (13 sensitive, 130 non-sensitive) that each other could not correctly classify.

Discussion:
The Mistral and Llama 2 models are both implemented using the transformer decoder architecture. However, our results show that Mistral outperforms Llama 2 under multiple conditions, except our base prompts which perform worse than naive classifiers. Llama 2’s downfall is its extremely high recall (>0.9) and low specificity for prompt techniques Base, PDC and CG, demonstrating the model is highly confused during classifying sensitive personal information in SARA. Whereas, Mistral is more rationale at detecting sensitive personal information [example where it acknowledges names but realises there is no protected information alongside these – qualitative example of Llama 2 incorrect vs Mistral correct]. We suggest that the Mistral model/LLM has better rationality/decision-making capabilities when identifying sensitive personal information, and that Llama 2 is over-protective of personally identifiable information such [/as it is seen] that is more likely to reason that names which are inherent to emails, hence should be deemed non-sensitive for most documents (for useful sensitivity classifiers), are regarded as sensitive personal information. We do not have insights into the training data of these models due to the current competitive nature of pre-training LLMs; however, Meta has aimed to censor toxic generation from their models, hence may be over-protective to attributes such as names.
[In fact, many models suffer from high recall of the sensitive class. We believe this is due to easily detectable PII within emails such as full names, which are regarded as sensitive information (within the GDPR [CITE?]). Few-shot improves on this a lot.] [This is the opposite for Flan-T5, which uses the different encoder-decoder architecture instead of a decoder-only and shows significantly higher specificity (test statistic). This is interesting as Flan-T5's maximum context window is eight times smaller than our decoder-only models, meaning that context is likely lost as documents are broken up given longer emails and email threads (shown by Table with the number of documents fed to each model due to context window limitations). It gets many more documents – are the short segments including very sensitive personal information – classifies all as purely personal. Sequence-to-sequence models should not be as performant as our causal language models.]


\textbf{Comparison to previous work.} We compare our zero-shot generative LLMs to naive baselines and following methods of traditional machine learning techniques used in previous work [CITE]. From Table \ref{tab:ml} we observe that LLMs given little explanation within the prompt about the task perform similar to or worse than our random baselines (stratified random sampling and most frequent). This suggests that these models pre-trained knowledge of personal sensitive information does not entirely align with the sensitive personal information present in emails such as SARA. With our best performing system prompt, we elicit more accurate responses, and these generative LLMs with prompt engineering do perform statistically significantly better than both baselines (where Mistral obtains a 0.6790 BAC, (test statistics X and Y for stratified random sampling and most frequent respectively)). Machine learning techniques logistic regression and SVM perform similar to our best performing model settings, with (BAC and F1). There is/not a statistical difference between X and Y. Furthermore, supervised machine learning strategies are rarely confused given non-sensitive documents, whereas our zero-shot strategy with generative LLMs overpredict sensitivity. This is shown by the much lower TPR when using our generative LLMs.

Discussion: Reiterate I believe these false positives are due to zero-shotting and pre-trained knowledge of PII.
[Furthermore, there is a large cost increase to use LLMs over machine learning strategies.]

Overall, in answer to RQ1, we say that generative LLMs can be useful for detecting sensitive personal information. However, their utility in email collections should be considered carefully due to their pre-trained knowledge/understanding of PII and (sensitive) personal information.

\subsection{RQ2. What is the effect of prompt engineering strategies on the effectiveness of zero-shot classification with generative LLMs?}
We now analyse the effectiveness of our prompt engineering techniques used to enhance generative LLMs with no fine-tuning. Table \ref{} shows the performance of our different prompt engineering techniques over different models.

Context Manager – Focusing on Mistral, we find that our additional context (about emails in work-related/professional collection) results in statistically significant improvements to classification effectiveness (shown in Table using McNemar’s tests comparing base to PDC and CG).

Discussion: Our discussion in RQ1 comparing a base classification instruction prompt to random baselines demonstrates it is necessary for context to be added detailing the types of documents the model will be given to classify. Prompt instructions are used similarly like features for what justifies an email that contains sensitive personal information, which has shown to enhance the model for classifying sensitive documents correctly[/more accurately]. Furthermore, we find using prompt instructions explaining other types of documents found within the collection further enhances sensitivity classifications, mitigating false positives and model confusion. This is shown by the improvements in our precision metrics from Mistral-PDC to Mistral-CG, and (hence their) F1 score (as well). We believe this contextual information helps to guide the model in classification alongside it’s pre-trained knowledge of PII and sensitive personal information by providing both informative focus to sensitive personal information, and further knowledge about the potential contents of the email documents. This can be used/utilised through PLM’s linguistic understanding [/ability to understand both semantic and context] to infer a final sensitive or non-sensitive (document) class.

We show that information about the email category is useful as messages that belong to non-sensitive categories (such as empty messages) but contain names, such as within metadata (header or signatures) or PII, were able to be correctly classified as non-sensitive. This is shown by comparison of genre confusion given the Hearst annotations for the 8 email coarse genres [confusion plot/heatmap - showing weights/heats of improvements]. From Figure \ref{fig} we see that most categories improved except personal in a professional context (sensitive) and empty message due to attachment (non-sensitive). Furthermore, emails discussing employment arrangements are regularly misclassified; where opinions about candidates are (often) stated between hiring managers [/others]. We believe/conclude this type of email document is similar discussing sensitive personal information but in a professional context and PLM struggle to identify these messages as non-sensitive. Overall, we conclude our use of the context manager pattern improved the effectiveness of using generative LLMs in zero-shot sensitivity classification.

\subsection{RQ3. What is the effect of other [from the literature/as seen in the literature] prompt engineering strategies on the effectiveness of sensitivity classification with generative LLMs?}

Few shot – It is apparent that few-shot can improve on the effectiveness of generative LLMs for sensitivity classification. We see significant improvements to our base prompting strategy with few-shot, where BAC increases by 9.06\% (from 0.4348 to 0.4742) for the Mistral model. Despite this improvement, this prompting strategy still performs poorly in sensitivity classification; performing worse than naive classifiers, and this is true across all models used in our experiments. We also see statistically significant improvements for the few-shot strategy with our PDC prompt (test statistic). Lastly, our CG prompt with few-shot varies Mistral’s effectiveness minorly by 1.95\% (and test statistic X demonstrating (in)significance); however, these additional few-shot examples does not hinder this best context manager/management prompting strategy, as both CG prompts perform better than our base prompt and PDC with and without few-shot.

[Flan-T5 is less successful at few-shot [/we do not include Flan-T5 using few-shot]. This model’s small context window makes it unsuccessful/incapable for experimenting with our prompt (engineering) and in-context learning scenario as we have many large documents.]

Discussion: We see the few-shot strategies improves sensitivity classification. [We have evaluated the few-shot prompt engineering techniques across multiple PLMs, which demonstrates prompt engineering can improve the effectiveness of their sensitivity classification.] These larger decoder-only models are able to include in-context examples. Furthermore, large context windows are inherent to decoder-only LLMs, as this architecture is more efficient at processing sequences compared to LLMs with an encoder (or encoder-decoder such as Flan-T5) because of the bidirectional context that is captured. Therefore, decoder-only LLMs are able to be feasibly used with large context windows that allows for successful few-shot (learning).

We included two examples in-context: one sensitive, and one non-sensitive. Furthermore, emails in the collection despite being non-sensitive can vary[/are relevant] across six other genres (as shown by Hearst and Mckechnie); therefore, it would be useful to investigate using examples that were not cherry-picked or selected via cosine similarity. [(Surprisingly,) Selected examples for few-shot that were most similar document to the document to be classified performed significantly worse compared to both zero-shot and cherry-picked examples that were clearly sensitive or non-sensitive [to a human reader/reviewer] (test statistic) [is it appropriate to use clearly – for a human reader the examples are clear]. [We suggest[/otherword like suppose more like a curious suggestion] that as emails could be structured [also structured semantically] the same way by individuals retrieving ‘best’ examples will be challenging [maybe rephrase – sounds like you couldnt do it].

As SARA is not a (very/an extremely) large collection we chose to find similarities between all documents using the cosine similarity; however, larger collections could employ retrieval models to find similar documents. Furthermore, ongoing research is investigating (novel) few-shot prompting strategies. Techniques such as DSPy’s bootstrapped few-shot which generate synthetic and representative few-shot samples could be explored. [This technique could be used to release prompts publicly as synthetic few-shot examples may be useful within other collections – bit of a reach]. Methods are being explored how to retrieve the best example prompts, and we believe that similar to our context manager pattern, it could be useful to demonstrate multiple examples of sensitive and non-sensitive emails.

For Chain-of-Thought, our models do not behave [outperform no-CoT] as we hypothesised. We believed taking reasoning steps/hops would elicit sensible self-explanations to better classify the email message as sensitive or non-sensitive given additional awareness/context of personal information within the email. There is a statistically significant difference between zero-shot and CoT prompts for Mistral and Mixtral models where the zero-shot prompt outperforms CoT techniques, shown by BAC lowering by 5.96\% and 4.24\% respectively. Interestingly, specificity/TNR is highest using CoT with these models at 0.7184 and 0.6829 respectively which is significantly different to the zero-shot prompt (test statistic – or insignificant). We have already found Llama 2 with our techniques has a demonstrated poor understanding of the sensitivity classification task and this prompting technique makes insignificant changes to this model; following the trend of overpredicting sensitivity [test statistic and remove if not true on analysis – seems true from inspections].


\begin{table*}[]
\begin{tabular}{@{}lllllllllll@{}}
\toprule
Model & Prompt & Accuracy & Precision & Recall & TPR & TNR & $F_{1}$ & $F_{2}$ & BAC & auROC \\ \midrule
Mistral & Base & 0.6042 & 0.0806 & 0.2092 & 0.2092 & 0.6604 & 0.1163 & 0.1585 & 0.4348 & 0.4348 \\
Mistral & PDC & 0.4892 & 0.1630 & 0.7500 & 0.7500 & 0.4521 & 0.2678 & 0.4359 & 0.6011 & 0.6011 \\
Mistral & CG & 0.6449 & 0.2195 & 0.7245 & 0.7245 & 0.6335 & 0.3369 & 0.4962 & 0.6790 & 0.6790 \\
Mistral & Base+Fewshot & 0.5966 & 0.1087 & 0.3112 & 0.3112 & 0.6372 & 0.1612 & 0.2268 & 0.4742 & 0.4742 \\
Mistral & PDC+Fewshot & 0.5534 & 0.1803 & 0.7296 & 0.7296 & 0.5283 & 0.2892 & 0.4534 & 0.6289 & 0.6289 \\
Mistral & CG+Fewshot & 0.6182 & 0.2069 & 0.7296 & 0.7296 & 0.6023 & 0.3224 & 0.4847 & 0.6660 & 0.6660 \\ \bottomrule
\end{tabular}
\end{table*}


\begin{table*}[]
\begin{tabular}{@{}cllllllll@{}}
\toprule
\multicolumn{1}{l}{\textbf{Model}} &
  \textbf{Prompt} &
  \textbf{Accuracy} &
  \textbf{Precision} &
  \textbf{TPR} &
  \textbf{TNR} &
  \textbf{$F_{1}$} &
  \textbf{$F_{2}$} &
  \textbf{BAC} \\ \midrule
\multirow{7}{*}{Mistral} & Base    & 0.6042 & 0.0806 & 0.2092 & 0.6604 & 0.1163 & 0.1585 & 0.4348 \\
                         & PDC     & 0.4892 & 0.1630 & 0.7500 & 0.4521 & 0.2678 & 0.4359 & 0.6011 \\
                         & CG      & 0.6449 & 0.2195 & 0.7245 & 0.6335 & 0.3369 & 0.4962 & 0.6790 \\
                         & Base+FS & 0.5966 & 0.1087 & 0.3112 & 0.6372 & 0.1612 & 0.2268 & 0.4742 \\
                         & PDC+FS  & 0.5534 & 0.1803 & 0.7296 & 0.5283 & 0.2892 & 0.4534 & 0.6289 \\
                         & CG+FS   & 0.6182 & 0.2069 & 0.7296 & 0.6023 & 0.3224 & 0.4847 & 0.6660 \\
                         & CG+CoT  & 0.6938 & 0.2082 & 0.5204 & 0.7184 & 0.2974 & 0.4003 & 0.6194 \\
\hline
\multirow{7}{*}{Mixtral} & Base    & 0.5470 & 0.0967 & 0.3163 & 0.5798 & 0.1481 & 0.2175 & 0.4481 \\
                         & PDC     & 0.2332 & 0.1345 & 0.9490 & 0.1313 & 0.2356 & 0.4292 & 0.5402 \\
                         & CG      & 0.3723 & 0.1574 & 0.9286 & 0.2932 & 0.2692 & 0.4691 & 0.6109 \\
                         & Base+FS & 0.7814 & 0.1186 & 0.1173 & 0.8759 & 0.1179 & 0.1176 & 0.4966 \\
                         & PDC+FS  & 0.4701 & 0.1621 & 0.7806 & 0.4260 & 0.2684 & 0.4427 & 0.6033 \\
                         & CG+FS   & 0.6741 & 0.2272 & 0.6735 & 0.6742 & 0.3398 & 0.4835 & 0.6738 \\
                         & CG+CoT  & 0.6544 & 0.1692 & 0.4541 & 0.6829 & 0.2465 & 0.3397 & 0.5685 \\
\hline
\multirow{6}{*}{Llama 2} & Base    & 0.1518 & 0.1211 & 0.9286 & 0.0414 & 0.2142 & 0.3979 & 0.4850 \\
                         & PDC     & 0.1842 & 0.1274 & 0.9490 & 0.0755 & 0.2246 & 0.4144 & 0.5122 \\
                         & CG      & 0.1665 & 0.1270 & 0.9694 & 0.0522 & 0.2246 & 0.4167 & 0.5108 \\
                         & Base+FS & 0.7579 & 0.0925 & 0.1071 & 0.8505 & 0.0993 & 0.1039 & 0.4788 \\
                         & PDC+FS  & 0.5388 & 0.1679 & 0.6837 & 0.5181 & 0.2696 & 0.4235 & 0.6009 \\
                         & CG+FS   & 0.6334 & 0.2009 & 0.6531 & 0.6306 & 0.3073 & 0.4504 & 0.6418 \\
\hline
\multirow{6}{*}{FLAN-T5} & Base    & 0.8583 & 0.1860 & 0.0408 & 0.9746 & 0.0669 & 0.0484 & 0.5077 \\
                         & PDC     & 0.8545 & 0.3922 & 0.3061 & 0.9325 & 0.3438 & 0.3202 & 0.6193 \\
                         & CG      & 0.7973 & 0.2970 & 0.4592 & 0.8454 & 0.3607 & 0.4140 & 0.6523 \\
                         & Base+FS & 0.8748 & 0.3333 & 0.0051 & 0.9985 & 0.0101 & 0.0064 & 0.5018 \\
                         & PDC+FS  & 0.8463 & 0.3231 & 0.2143 & 0.9361 & 0.2577 & 0.2298 & 0.5752 \\
                         & CG+FS   & 0.7948 & 0.2510 & 0.3265 & 0.8614 & 0.2838 & 0.3080 & 0.5940 \\ \cmidrule(l){1-9} 
\end{tabular}
\end{table*}

