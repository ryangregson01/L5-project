\section{Results}

\begin{table*}[!h]
\caption{Results. Significance testing where p<0.05. $\dagger$ shows comparison to Base, and $\ddagger$ shows comparisons to non-fewshot prompts. Note p<0.01 for all current significance holds true except Mixtral with SensCat (compared to Base) with 0.0137... . [Underlined values are per prompt - this could be changed per model - improve readability.]}
\label{table:results}
\begin{tabular}{@{}lclccccccc@{}}
\toprule
Prompt &
  &
  Model &
  Accuracy &
  Precision &
  TPR &
  TNR &
  $F_{1}$ &
  $F_{2}$ &
  BAC \\ \midrule
\multirow{3}{*}{Base} &
  &
  Mistral &
  {\ul 0.7891} &
  {\ul 0.1792} &
  0.1939 &
  {\ul 0.8737} &
  0.1863 &
  0.1908 &
  0.5338 \\
 &
  &
  Mixtral &
  0.5006 &
  0.1349 &
  0.5561 &
  0.4927 &
  0.2171 &
  0.3423 &
  0.5244 \\
 &
  &
  Llama 2 &
  0.2719 &
  0.1374 &
  {\ul \textbf{0.9184}} &
  0.1800 &
  {\ul 0.2390} &
  {\ul 0.4298} &
  {\ul 0.5492} \\ \cmidrule(l){3-10}
\multirow{3}{*}{SensCat} &
  $\dagger$ \hspace{0.46em} &
  Mistral &
  {\ul 0.8653} &
  {\ul 0.4216} &
  0.2194 &
  {\ul 0.9572} &
  {\ul 0.2886} &
  0.2427 &
  0.5883 \\
 &
  $\dagger$ \hspace{0.46em} &
  Mixtral &
  0.5299 &
  0.1699 &
  {\ul 0.7143} &
  0.5036 &
  0.2745 &
  {\ul 0.4353} &
  {\ul 0.6090} \\
 &
  $\dagger$ \hspace{0.46em} &
  Llama 2 &
  0.5299 &
  0.1699 &
  0.7143 &
  0.5036 &
  0.2745 &
  0.4353 &
  0.6090 \\ \cmidrule(l){3-10}
\multirow{3}{*}{SensCat+NonSensCat} &
  $\dagger$ \hspace{0.46em} &
  Mistral &
  {\ul 0.8564} &
  {\ul 0.4038} &
  0.3214 &
  {\ul 0.9325} &
  {\ul 0.3580} &
  0.3351 &
  0.6270 \\
 &
  $\dagger$ \hspace{0.46em} &
  Mixtral &
  0.5616 &
  0.1873 &
  {\ul 0.7551} &
  0.5341 &
  0.3002 &
  {\ul 0.4701} &
  {\ul 0.6446} \\
 &
  $\dagger$ \hspace{0.46em} &
  Llama 2 &
  0.6296 &
  0.1670 &
  0.4949 &
  0.6488 &
  0.2497 &
  0.3553 &
  0.5718 \\ \cmidrule(l){3-10}
\multirow{3}{*}{Base+SensDesc} &
  &
  Mistral &
  {\ul 0.7827} &
  {\ul 0.2148} &
  0.2806 &
  {\ul 0.8541} &
  {\ul 0.2434} &
  0.2644 &
  {\ul 0.5674} \\
 &
  $\dagger$ \hspace{0.46em} &
  Mixtral &
  0.5394 &
  0.1411 &
  0.5306 &
  0.5406 &
  0.2229 &
  0.3419 &
  0.5356 \\
 &
  &
  Llama 2 &
  0.2605 &
  0.1350 &
  {\ul 0.9133} &
  0.1676 &
  0.2352 &
  {\ul 0.4242} &
  0.5404 \\ \cmidrule(l){3-10}
\multirow{3}{*}{SensCat+SensDesc} &
  $\dagger$ \hspace{0.46em} &
  Mistral &
  {\ul 0.8653} &
  {\ul 0.4467} &
  0.3418 &
  {\ul 0.9398} &
  {\ul 0.3873} &
  0.3587 &
  {\ul 0.6408} \\
 &
  $\dagger$ \hspace{0.46em} &
  Mixtral &
  0.6436 &
  0.1973 &
  0.6071 &
  0.6488 &
  0.2979 &
  {\ul 0.4290} &
  0.6280 \\
 &
  $\dagger$ \hspace{0.46em} &
  Llama 2 &
  0.5273 &
  0.1667 &
  {\ul 0.6990} &
  0.5029 &
  0.2692 &
  0.4265 &
  0.6009 \\ \cmidrule(l){3-10}
\multirow{3}{*}{SensCat+NonSensCat+SensDesc} &
  $\dagger$ \hspace{0.46em} &
  Mistral &
  {\ul 0.8571} &
  {\ul 0.4249} &
  0.4184 &
  {\ul 0.9194} &
  {\ul \textbf{0.4216}} &
  0.4197 &
  {\ul 0.6689} \\
 &
  $\dagger$ \hspace{0.46em} &
  Mixtral &
  0.6404 &
  0.2100 &
  {\ul 0.6837} &
  0.6343 &
  0.3213 &
  {\ul \textbf{0.4712}} &
  0.6590 \\
 &
  $\dagger$ \hspace{0.46em} &
  Llama 2 &
  0.5521 &
  0.1460 &
  0.5357 &
  0.5544 &
  0.2295 &
  0.3493 &
  0.5451 \\ \midrule %\cmidrule(l){3-10}
  \midrule
\multirow{3}{*}{Base+FS} &
  $\dagger$ $\ddagger$ &
  Mistral &
  0.8520 &
  {\ul 0.3609} &
  0.2449 &
  0.9383 &
  {\ul 0.2918} &
  0.2617 &
  0.5916 \\
 &
  $\dagger$ $\ddagger$ &
  Mixtral &
  0.8259 &
  0.2926 &
  {\ul 0.2806} &
  0.9035 &
  0.2865 &
  {\ul 0.2829} &
  {\ul 0.5920} \\
 &
  $\dagger$ $\ddagger$ &
  Llama 2 &
  {\ul 0.8564} &
  0.2222 &
  0.0612 &
  {\ul 0.9695} &
  0.0960 &
  0.0716 &
  0.5154 \\ \cmidrule(l){3-10}
\multirow{3}{*}{SensCat+FS} &
  $\dagger$ $\ddagger$ &
  Mistral &
  0.8405 &
  0.3799 &
  {\ul 0.4439} &
  0.8970 &
  {\ul 0.4094} &
  {\ul 0.4294} &
  {\ul 0.6704} \\
 &
  $\dagger$ $\ddagger$ &
  Mixtral &
  {\ul 0.8640} &
  {\ul 0.4392} &
  0.3316 &
  0.9398 &
  0.3779 &
  0.3487 &
  0.6357 \\
 &
  $\dagger$ $\ddagger$ &
  Llama 2 &
  0.8634 &
  0.3908 &
  0.1735 &
  {\ul 0.9615} &
  0.2403 &
  0.1952 &
  0.5675 \\ \cmidrule(l){3-10}
\multirow{3}{*}{SensCat+NonSensCat+FS} &
  $\dagger$ \hspace{0.46em} &
  Mistral &
  0.8437 &
  0.3821 &
  {\ul 0.4133} &
  0.9049 &
  {\ul 0.3971} &
  {\ul 0.4066} &
  {\ul 0.6591} \\
 &
  $\dagger$ $\ddagger$ &
  Mixtral &
  {\ul 0.8736} &
  {\ul 0.4835} &
  0.2245 &
  0.9659 &
  0.3066 &
  0.2514 &
  0.5952 \\
 &
  $\dagger$ $\ddagger$ &
  Llama 2 &
  0.8717 &
  0.4605 &
  0.1786 &
  {\ul 0.9702} &
  0.2574 &
  0.2035 &
  0.5744 \\ \cmidrule(l){3-10}
\multirow{3}{*}{Base+SensDesc+FS} &
  $\dagger$ $\ddagger$ &
  Mistral &
  0.8374 &
  {\ul 0.3469} &
  {\ul 0.3469} &
  0.9071 &
  {\ul 0.3469} &
  {\ul 0.3469} &
  {\ul 0.6270} \\
 &
  $\dagger$ $\ddagger$ &
  Mixtral &
  {\ul 0.8602} &
  0.2931 &
  0.0867 &
  {\ul 0.9702} &
  0.1339 &
  0.1010 &
  0.5285 \\
 &
  $\dagger$ $\ddagger$ &
  Llama 2 &
  0.8488 &
  0.1613 &
  0.0510 &
  0.9623 &
  0.0775 &
  0.0591 &
  0.5066 \\ \cmidrule(l){3-10}
\multirow{3}{*}{SensCat+SensDesc+FS} &
  $\dagger$ $\ddagger$ &
  Mistral &
  0.8259 &
  0.3545 &
  {\ul 0.4847} &
  0.8745 &
  {\ul 0.4095} &
  {\ul 0.4515} &
  {\ul \textbf{0.6796}} \\
 &
  $\dagger$ $\ddagger$ &
  Mixtral &
  {\ul 0.8710} &
  {\ul 0.4667} &
  0.2500 &
  {\ul 0.9594} &
  0.3256 &
  0.2756 &
  0.6047 \\
 &
  $\dagger$ $\ddagger$ &
  Llama 2 &
  0.8532 &
  0.3267 &
  0.1684 &
  0.9507 &
  0.2222 &
  0.1864 &
  0.5595 \\ \cmidrule(l){3-10}
\multirow{3}{*}{SensCat+NonSensCat+SensDesc+FS} &
  $\dagger$ $\ddagger$ &
  Mistral &
  0.8342 &
  0.3663 &
  {\ul 0.4541} &
  0.8882 &
  {\ul 0.4055} &
  {\ul 0.4333} &
  {\ul 0.6712} \\
 &
  $\dagger$ $\ddagger$ &
  Mixtral &
  {\ul \textbf{0.8761}} &
  {\ul \textbf{0.5079}} &
  0.1633 &
  {\ul \textbf{0.9775}} &
  0.2471 &
  0.1889 &
  0.5704 \\
 &
  $\dagger$ $\ddagger$ &
  Llama 2 &
  0.8494 &
  0.3566 &
  0.2602 &
  0.9332 &
  0.3009 &
  0.2751 &
  0.5967 \\ \cmidrule(l){3-10}
\multirow{3}{*}{SensCat+NonSensCat+SensDesc+CoT} &
  $\dagger$ $\ddagger$ &
  Mistral &
  0.5667 &
  0.1373 &
  {\ul 0.4694} &
  0.5806 &
  {\ul 0.2125} &
  {\ul 0.3164} &
  0.5250 \\
 &
  $\dagger$ $\ddagger$ &
  Mixtral &
  {\ul 0.7395} &
  {\ul 0.1677} &
  0.2755 &
  {\ul 0.8055} &
  0.2085 &
  0.2441 &
  {\ul 0.5405} \\
 &
  $\dagger$ $\ddagger$ &
  Llama 2 &
  0.3259 &
  0.1236 &
  0.7245 &
  0.2692 &
  0.2112 &
  0.3673 &
  0.4969 \\ \cmidrule(l){1-10} 
\end{tabular}
\end{table*}

\subsection{RQ1. How effective are generative LLMs for identifying sensitive personal information within emails.}

We first analyse the effectiveness of different generative LLMs. Table \ref{table:results} contains the results of Mistral, Mixtral, and Llama 2 for SARA. All of these models are instruction-finetuned, making them appropriate to use for classification and reasoning-based tasks. We observe that Mistral models outperform Llama 2. Mistral obtains its highest BAC and F\textsubscript{2} with prompt $SensCat+SensDesc+FS$ at 0.6796 BAC and 0.4515 F\textsubscript{2}, and Mixtral with prompt $SensCat+NonSensCat+SensDesc$ achieves 0.6590 BAC and 0.4712 F\textsubscript{2}, whereas Llama 2 (with prompt $SensCat$) achieves 0.6090 BAC and 0.4353 F\textsubscript{2} at its best. Mistral's performance over Llama 2 is also in agreement with other tasks [CITE]. We conclude Mistral models are better in our task of identifying sensitive personal information. /[We conclude Mistral is (statistically) significantly better than Llama 2 for identifying sensitive personal information within our email collection using McNemarâ€™s test (statistic value - is it feasible to compare differing models and promtps - two changed variables when we do it across best model settings.)]. Interestingly, the Mixtral model that is larger than Mistral does not outperform Mistral in sensitivity classification. We believe there are some differences between these models regarding the overall effectiveness of sensitivity identification. Each model with its best prompt settings have statistically significant differences (McNemar's p-value of $1.6659\cdot{10^{-41}}$) [if allowed with model and prompt change], where they disagree on 496 documents: Mistral correctly classifying 394 documents (9 sensitive, 385 non-sensitive) and Mixtral correctly classifying 103 different documents (48 sensitive, 54 non-sensitive) that each other could not correctly classify. This agrees with/explains Mixtral's higher F\textsubscript{2}-score as the more recall-oriented model is protecting more sensitive documents. [Maybe for prompt engineering few-shot - more focused RQ - Furthermore, we see from Table \ref{table:results} that zero-shotting Mixtral 
with single context management prompt engineering techniques makes greater improvements to a base prompting approach compared to the other models/Mistral model. However, the addition of few-shot prompting reduces the quality of Mixtral's responses/classifications which is the opposite behaviour of Mistral. We see that Mixtral with few-shot prompting struggles to correctly identify sensitive documents, with recall (TNR) being (negatively) impacted [and significantly lowered].

The Mistral and Llama 2 models are both implemented using the transformer decoder architecture. However, our results show that Mistral outperforms Llama 2 under multiple conditions, except our $Base$ and $SensCat$ prompt. Llama 2 is highly confused during classifying sensitive personal information in SARA, and this is shown by low specificity (TNR) scores given zero-shot prompts. Mixtral's shortfalls follow the same trend as Llama 2; however, incorrect classifications are less severe with Mixtral. On the other hand, Mistral is more rationale at detecting sensitive personal information, and this is shown by the greater F\textsubscript{1}-score across all prompts except $Base$. We suggest that the Mistral LLM has better decision-making capabilities when identifying sensitive personal information, and that Llama 2 is over-protective of personally identifiable information as analysis of responses show that Mistral can deduce that personal information is not always sensitive personal information, which Llama 2 struggles with more. [â€“ qualitative example of Llama 2 incorrect vs Mistral correct/ref analysis]. This is important as models reasoning that names are sensitive personal information are less useful as sensitivity classifiers because they cannot distinguish personal information that is inherent to email documents and hence should be deemed as non-sensitive for most documents to sensitive personal information of interest. We do not have insights into the training data of these models due to the current competitive nature of pre-training LLMs; however, Meta has aimed to censor toxic generation from their models, hence may be over-protective to attributes such as names.

\textbf{Comparison to previous work.} We compare our zero-shot generative LLMs to naive baselines and traditional machine learning techniques used in previous work \cite{mckechnie2024sara}. From Table \ref{table:ml} we observe that LLMs given little explanation within the prompt about the task perform slightly better than our baselines stratified random sampling and most frequent classification. This slight improvement suggests that these models pre-trained knowledge of personal sensitive information does not entirely align with the sensitive personal information present in emails such as SARA; however, they are more useful than naive approaches. [CoT L2 is worse than random - comment in analysis.]. With our best performing system prompt, we achieve more accurate sensitivity classifications, where our generative LLMs alongside prompt engineering do perform statistically significantly better than both baselines. Machine learning techniques logistic regression and SVM perform similar to our best performing model settings, with (BAC and F2-scores). There is/not a statistical difference between X and Y. Furthermore, supervised machine learning strategies are rarely confused given non-sensitive documents, whereas our zero-shot strategy with generative LLMs often overpredict sensitivity. This is shown by the much lower TPR when using our generative LLMs. 

[Reiterate I believe these false positives are due to zero-shotting and pre-trained knowledge of PII. Furthermore, there is a large cost increase to use LLMs over machine learning strategies.]

Overall, in answer to RQ1, we say that generative LLMs can be useful for detecting sensitive personal information. However, their utility in email collections should be considered carefully due to their pre-trained understanding of PII and (sensitive) personal information.

\subsection{RQ2. What is the effect of the context manager prompt engineering strategy on the effectiveness of zero-shot classification with generative LLMs?}
We now analyse the effectiveness of introducing context within the prompt to enhance zero-shot generative LLMs. Table \ref{table:results} shows the performance of our different prompt engineering techniques, where we focus on introduced context with the prompt strategies $SensCat$, $NonSensCat$ and $SensDesc$ without any few-shot prompting. We see that our contextual phrases within prompts does improve the classification effectiveness, with significant differences across all strategies except $SensDesc$ with Mistral and Llama 2. However, the addition of $SensDesc$ with $SensCat$ and $SensCat+NonSensCat$ does further improve the classification performance of these prompts compared to the exclusion of $SensDesc$ with Mistral and Mixtral; therefore, we suggest this context is useful. For Mistral and Mixtral, the combination of all three contextual prompts achieves the best classification performance, and Llama 2 performs best with just the $SensCat$ context. Therefore, we conclude our additional context about sensitive personal information and email categories suitable for Enron's email collection lead to statistically significant improvements to zero-shot sensitivity classification. 

We believe prompt instructions can be used similarly to features [/as natural language features] for what justifies sensitive personal information within an email, because this context has improved classification of sensitive documents. This effect is true for Mistral and is shown by the increasing TPR. Furthermore, we find using prompt instructions explaining other types of documents that would be present within the document collection further enhances sensitivity classification. We see this from the increase of TNR compared to approaches without email category context and improvements to the precision metrics (mitigating model confusion). We conclude this contextual information helps to guide the model in classification alongside itâ€™s pre-trained knowledge of sensitive personal information by providing both informative focus to sensitive personal information, and further knowledge about the potential contents of the email documents. This can be used/utilised through LLMs linguistic understanding [/ability to understand both semantic and context] to infer a final sensitive or non-sensitive (document) class that agrees with both its pre-trained understanding and our provided system context. 

We have shown that information about the email category is useful as messages that belong to sensitive and non-sensitive categories improve given this additional context. This is also shown by a comparison of category confusion given the Hearst annotations for the 8 email coarse categories [confusion plot/heatmap - showing weights/heats of improvements]. From Figure \ref{fig:heatmap} we see that all categories improved compared to the base prompt. Interestingly, the inclusion of non-sensitive email categories improves the classification of sensitive email documents, where we believed this contextual information would in fact mitigate false positives by exposing the model to be aware of the type of document it is classifying, which in turn may have introduced more true (and false) negatives. Furthermore, emails discussing employment arrangements are proportionally the most misclassified non-sensitive email category; where opinions about candidates are (often) stated between hiring managers [/others]. We believe/conclude this type of email document is similar to discussing sensitive personal information but in a professional context and the LLMs struggle to identify these messages as non-sensitive.

Overall, we conclude our use of the context manager pattern improved the effectiveness of using generative LLMs in zero-shot sensitivity classification.

\subsection{RQ3. What is the effect of other prompt engineering strategies on the effectiveness of sensitivity classification with generative LLMs?}

To answer our final research question, we augment our prompts with popular methods seen in literature: few-shot prompting and chain-of-thought prompting.

\subsubsection{What is the effect of few-shot prompting on the effectiveness of sensitivity classification with generative LLMs?}

It is clear that few-shot improves the effectiveness of Mistral for identifying sensitive personal information. From Table \ref{table:results}, for Mistral we see every prompt except $SensCat+NonSensCat$ with few-shot makes a statistically significant improvement (to comparable zero-shot prompts), and all BAC and F\textsubscript{2} values are seen to increase given few-shot. For Mixtral and Llama 2, the addition of few-shot makes statistically significant differences from the pseudo-baseline zero-shot prompt; however, few-shot does not improve every prompt instance of these models. Mixtral and Llama 2 suffered from low specificity with our zero-shot prompts; however, with few-shot prompting the [/the opposite is true where] TNR is extremely high (>0.9 for all prompts). With a high TNR, one of our main metrics F\textsubscript{2} is extremely low for these model-prompt settings; therefore, we conclude few-shot with Mixtral and Llama 2 is not effective for identifying sensitive personal information.

We see that few-shot prompt engineering consistently improves sensitivity classification with Mistral only. Therefore, we conclude this model is more effective than Llama 2 and Mixtral with few-shot prompting. Furthermore, we also speculate that the poor performance with the other models using few-shot could be for reasons such as poor example choices and prompt structure[/example ordering].

We included two examples in-context: one sensitive, and one non-sensitive. As the second example was always non-sensitive and our causal LLMs only use/[have access to] previous context, prioritising more recent tokens, the fact that an answer with '[``non-personal"]' appears more recently may be confusing the model. [Post-study flipping them]. As messages are reasonably long, hence the examples are long, creating a greater distance/separation/window before demonstrated answers, this is a limitation of using multiple examples with few-shot prompting. [I would say I hypothesised that multiple examples should have been important/useful though].

As well as example ordering, the choice of our examples could influence our results. Emails in the collection that are non-sensitive can vary across [/are relevant to] six other categories (as shown by Hearst \cite{hearst2005teaching} and Mckechnie \cite{mckechnie2024sara}, and used by our context manager prompting strategy); therefore, it would be useful to investigate using examples of multiple sensitive and non-sensitive email categories could improve the effectiveness of sensitivity classification compared to examples that have been cherry-picked or selected via cosine similarity. [(Surprisingly,) Selected examples for few-shot that were most similar document to the document to be classified performed significantly worse compared to both zero-shot and cherry-picked examples that were clearly sensitive or non-sensitive to a human reader (test statistic).] [We consider that as emails could be structured [also structured semantically] the same way by individuals retrieving â€˜bestâ€™ examples will be challenging].

As SARA is not a (very/an extremely) large collection we chose to find similarities between all documents using the cosine similarity; however, larger collections could employ retrieval models to find similar documents. Furthermore, ongoing research is investigating (novel) few-shot prompting strategies. Techniques such as DSPyâ€™s bootstrapped few-shot which generate synthetic and representative few-shot samples could be explored. [This technique could be used to release prompts publicly as synthetic few-shot examples may be useful within other collections]. Methods are being explored how to retrieve the best example prompts, and we believe that similar to our context manager pattern, it could be useful to demonstrate multiple examples of sensitive and non-sensitive emails.

\subsubsection{What is the effect of chain-of-thought on the effectiveness of sensitivity classification with generative LLMs?}

For Chain-of-Thought (CoT), our models do not behave as we hypothesised, performing worse than no CoT. We believed taking reasoning steps/hops would elicit sensible self-explanations to better classify the email message as sensitive or non-sensitive given additional awareness/context of personal information within the email. 

There is a statistically significant difference between zero-shot and CoT prompts for all of our models where the zero-shot prompt outperforms CoT techniques, shown by BAC lowering by 14.39\%, 11.85\% and 4.82\% for Mistral, Mixtral and Llama 2 respectively. In fact, CoT for Mistral and Llama 2 performs worse than the $Base$ prompt; Llama 2 even performing worse than a naive classifier. Interestingly, recall is still high using CoT with these models, and our analysis shows our self-reasoning step causes more model confusion compared to the exclusion of thought hops to describe (sensitive) personal information.

