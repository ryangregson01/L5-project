\section{Analysis}

\begin{figure*}
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\linewidth]{figures/mist-zeroshot.pdf}
        \caption{Mistral - Zero-shot}
        \label{fig:model1_zero}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\linewidth]{figures/mixt-zeroshot.pdf}
        \caption{Mixtral 2 - Zero-shot}
        \label{fig:model2_zero}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\linewidth]{figures/l27b-zeroshot.pdf}
        \caption{Model 3 - Zero-shot}
        \label{fig:model3_zero}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\linewidth]{figures/mist-fewshot.pdf}
        \caption{Model 1 - Few-shot}
        \label{fig:model1_few}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\linewidth]{figures/mixt-fewshot.pdf}
        \caption{Model 2 - Few-shot}
        \label{fig:model2_few}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\linewidth]{figures/l27b-fewshot.pdf}
        \caption{Model 3 - Few-shot}
        \label{fig:model3_few}
    \end{subfigure}
    \hfill
    \caption{Comparative results of Zero-shot and Few-shot learning across three models.}
    \label{fig:gains}
\end{figure*}

Our results show that our prompting strategies do improve sensitivity classification, where McNemar’s statistical tests show these strategies produce statistically significantly different classifications from a base prompting strategy. For the best performing model, Mistral, these are significant improvements in sensitivity classification. We see our gains given our prompting strategies is greater than our losses compared to a baseline with Mistral (Figure \ref{fig:model1_zero}). Furthermore, few-shot alongside our prompting strategies with Mistral demonstrate gains are correctly identifying sensitive documents, albeit at the cost of now misclassifying some non-sensitive documents as sensitive (Figure \ref{fig:model1_few}). As our objective aims to identify and protect sensitive personal information, we can justify that Mistral’s performance makes it a feasible sensitivity classifier. Analysis of the gains made by Mixtral with our prompting strategies shows encouraging improvements to sensitivity classification, where improvements are made to both non-sensitive and sensitive correct classification when zero-shot (Figure \ref{fig:model2_zero}). However, with few-shot applied to Mixtral, the losses in classifying sensitive documents are noticeably poor, where the model is much more likely to classify non-sensitive documents correctly instead (Figure \ref{fig:model2_few}). Finally, (Figure \ref{fig:model3_zero} and Figure \ref{fig:model3_few}) show Llama 2’s significant issues with correctly identifying sensitive personal information, and our prompt engineering efforts improve non-sensitive classifications. However, the high TPR of Llama 2 $Base$ gives reason for the loss in correctly identified sensitive documents.

\subsection{Misclassified documents} 

\subsubsection{Entirely Misclassified} 
\begin{table}[!htb]
\caption{Entirely misclassified documents}
\label{table:all_wrong}
\begin{tabular}{p{1.2in}p{1.8in}}
\hline
Email Content                                & Model Response                                                                                                           \\ \hline
http://www.pserc-.wisc.edu/index\_-home.html &
  {[}“non-personal”{]}. This message appears to be a URL and does not contain any personal information.            \\ \hline
http://www.ssrn-.com/                        & {[}“non-personal”{]}. The message does not contain any personal information or comments related to work or employees. It is a simple URL link. \\ \hline
http://www.housing-finance.org/Country-\%20Specific\%20Inform-ation/country\_specific-\_information.htm &
  {[}“non-personal”{]}. This message appears to be a URL and does not contain any personal information. \\ \hline
\end{tabular}
\end{table}

From Table \ref{table:misclassified}, we see a larger proportion of sensitive documents are misclassified across all classifiers. The Llama 2 model identifies more sensitive documents over the variable prompts; however, we believe this model is still not as sensible as Mistral because given our Base prompt Llama 2 achieves an extremely high TPR of 0.9184, whereas the inclusion of few-shot appears to flip most predictions, where Table \ref{table:results} shows Llama 2 with Base+FS with an extremely high TNR of 0.9695 and low TPR of 0.0960. These two prompts, base and base augmented with few-shot already covers many documents lowering the number of fully misclassified documents and demonstrating Llama 2’s lack of confidence at classifying sensitive documents confidently when zero-shot. Moreover, we have 3 sensitive documents that are entirely misclassified by all model-prompt settings. Inspecting these 3 emails, we see they are each single message threads where the message body is only a shared link. Hearst’s Enron email annotations categorise these emails as personal but in a professional context. Our verbose responses of our best classifier, Mistral with $S\_EC+NS\_EC+PS+FS$ (which also correctly identifies the most sensitive documents across all prompt strategies with Mistral), are shown in Table \ref{table:all_wrong}. Other prompt-model settings with verbose generation for these documents respond similarly; stating URLs are non-sensitive, or that they have not been given enough information to conclude there is sensitive personal information within the message, hence justify this as non-personal. We note some verbose responses were able to correctly identify SSRN is an acronym for ‘Social Science Research Network’ but did not show any other interesting understanding beyond this. This shows that LLMs with limited surrounding context given these very short emails could benefit from more external context. We notice an interesting feature within the message header when inspecting these full documents: the sender and receiver are the same person using a different email address (from j.kaminski@enron.com, to vkaminski@aol.com), which are addresses owned by Vincent Julian Kaminski, a managing director at Enron \footnote{\url{https://en.wikipedia.org/wiki/Vincent_Kaminski}}. There are two other very similar emails, also sent by Vincent to himself and containing a single hyperlink: \textit{`http://www.vaionl-ine.it'} and \textit{`http://www.housingfinance.org/Country\%20Spec-ific\%-20Information/Fact\%20book\%20(e)\%2099.pdf'}, which have misclassification rates of 0.9487. Llama 2 with Base prompt and Base+PS make correct predictions for these hyperlinks; however, we have concluded that Llama 2 given these prompting strategies lacks the strength of a successful sensitivity classifier by not identifying the nuanced sensitive personal information as both experimental settings achieve an extremely high TPR and low precision meaning nearly every document is classified as sensitive. However, both strategies also do not include our guided definition of sensitive personal information (S\_EC), which may allow them to select sensitive information by not being instructed about the sensitive categories of interest within this work-related collection.

From our inspection of entirely misclassified documents, we suggest that email header metadata could be useful for classifying sensitive personal information; in particular, the email correspondents. Further context could be given to the model, such as the number of correspondents and their relationships if this is known or mined in advance. However, from our CoT experiment we see realising personal information such as names can cause confusion; therefore, definitions of relationships would need to not overwhelm the context; for example, strong relationships may influence the model. We also conclude that sensitive content can be embedded within email documents that cannot be identified from a purely textual perspective, such as URLs. We note there are also documents with JPGs that LLMs often misclassified, not realising they were sensitive. Therefore, we suggest awareness of non-textual elements could be important in correctly identifying sensitive information.

\subsubsection{Top 50 misclassified documents}
We evaluate the top 50 overall misclassified documents (where other models top 50 misclassified messages share the overall top 5 with Mistral at 31, Mixtral at 30 and Llama 2 at 20). From Table \ref{table:misclassified} we see there are 8 non-sensitive documents misclassified and 42 sensitive documents within the top 50 misclassified messages.

Non-sensitive documents that are misclassified discuss company business and strategy (which is the most common non-sensitive category) and employment arrangements. Moreover, the employment arrangement emails make direct criticisms and opinions about individuals. Inspecting part of one of Mistral’s verbose responses: \textit{``it is important to note that the message contains negative comments about specific individuals, which may be considered sensitive or confidential if shared outside of a small circle of trusted colleagues"} [/ref to appendix figure], demonstrates that our prompting strategy to provide context on sensitive personal information as defined by McKechnie’s utilisation of personal information within a professional context has confused the LLM \cite{mckechnie2024sara}. This sentiment towards a colleague has a non-sensitive ground truth, and our instructions to identify comments about colleagues and employee treatment have likely encouraged the LLM that these opinions are sensitive. The exclusion of our prompt explanation of sensitive categories, with prompt strategy $PS$, correctly identifies this email. However, we believe prompting with $S\_EC$ context provides important information to create more sensible sensitivity classifications as shown by the overall gains in prediction correctness (Figure \ref{fig:gains}).

\begin{table}[]
\centering
\caption{Top 50 Misclassified Documents}
\label{table:misclassified}
\begin{tabular}{@{}lcccc@{}}
\toprule
Model   & \multicolumn{2}{c}{All Wrong} & \multicolumn{2}{c}{Top 50} \\ \midrule
        & Non-Sens   & Sens   & Non-Sens  & Sens \\
Mistral & 6               & 45          & 6              & 44        \\
Mixtral & 15              & 28          & 18             & 32        \\
Llama 2 & 4               & 4           & 26             & 24        \\
All     & 0               & 3           & 8              & 42        \\ \bottomrule
\end{tabular}
\end{table}

There is a higher proportion of misclassified sensitive documents. From manually inspecting these documents, we see misclassifications of purely personal information within emails with little surrounding context. One highly misclassified purely personal message includes an email sent as a test [must go in Appendix \ref{APPENDIX}]. This email does not expose sensitive personal information, but is also not work-related, hence is treated as purely personal. Verbose responses show the keyword ‘Enron’ in the message creates a relationship to work. Therefore, our prompting strategy stating personal messages are not related to work ($S\_EC$) is not useful here. Within a larger collection, few-shot examples that are similar to these test messages may be useful for correctly classifying this type of message as personal. 

The majority of misclassified sensitive emails are related to work, and from our analysis. From our analysis a highly misclassified sensitive document containing the statement ‘I heard you were a big hit,’ within an email thread that should be treated as personal but in a professional context is not correctly classified. We are interested in exploring if these LLMs could utilise their pre-trained understanding and follow our instructions for sensitivity classification. Therefore, to assist zero-shot learning, we actively interact with the LLMs, evaluating if we can improve sensitivity classification for this document. Mistral can successfully explain that this statement is a form of sentiment towards a colleague: \textit{``The sentence``I heard you were a big hit" is an expression often used to convey that the person being referred to has been very successful or popular in a particular situation. The phrase ``a big hit" means that the person or thing in question has been well-received and has gained a lot of approval or admiration from others."}. We further embed this response as a hint within our system prompt; however, the classification is still incorrect with a verbose response explaining \textit{“It does not contain any sensitive personal information. The phrase ``I heard you were a big hit is” a compliment and does not reveal any personal information”}. Despite understanding this term is a comment about a person which aligns with our $S\_EC$ system context, the model still incorrectly classifies the message as non-sensitive. This non-automated approach demonstrates generative LLMs capabilities at describing the content within documents for user feedback; however, difficulty following our defined instructions limits the classification performance here.

Overall, we find from exploring the most misclassified documents demonstrates that pre-trained LLMs do not always fully understand how to interpret sensitive personal information that is within the professional context of email documents. We believe further exploration of instructions to interpret information about sensitive personal information would be useful for identifying this misclassified sensitive information.

