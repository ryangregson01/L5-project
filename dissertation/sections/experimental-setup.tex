\section{Experimental Setup}

The objective of our experiments is to see how effectively we can classify potentially sensitive documents using generative LLMs, and the impact of prompt engineering strategies for identifying sensitive personal information, i.e.,
\begin{itemize}
    \item RQ1: How effective are generative LLMs for identifying sensitive personal information within emails.
    \begin{itemize}
    \item Can generative LLMs feasibly improve on current sensitivity classification methods for sensitive personal information.
    \end{itemize}
    \item RQ2: What is the importance of prompt engineering for classifying sensitive personal information with generative LLMs.
    \begin{itemize}
    \item What is the effect of the context manager prompt engineering strategy on the effectiveness of zero-shot classification with generative LLMs?
    \item What is the effect of few-shot prompting on the effectiveness of sensitivity classification with generative LLMs?
    \item What is the effect of chain-of-thought on the effectiveness of sensitivity classification with generative LLMs? [/Does chain-of-thought elicit sensible reasoning about sensitive personal information.]
    \end{itemize}
\end{itemize}

\subsection{Dataset}
As the dataset for our experiments, we use the SARA collection \cite{mckechnie2024sara}. The SARA collection has many long and unstructured email threads; therefore, we preprocess these documents to create consistency between the documents and remove noise across the collection, giving us more robust input to our LLMs. Our preprocessing, shown by Figure [FIG] involves:
\begin{enumerate}
    \item Removing email headers, as we are interested in personal information found in the body of our messages.
    \item Removing email addresses from email thread metadata as this obvious PII caused model confusion in early experiments and we aim to focus on identifying sensitive personal information within the message.
    \item Removing all consecutive whitespace characters.
    \item Using simple preprocessing strategies to remove numerical values, punctuation and lowercase our documents \cite{rehurek2011gensim}.
    \item We do not remove any stop words from the documents as this is less representative of the training data seen by pretrained LLMs.
\end{enumerate}

Preprocessing leads us to remove 128 duplicate documents which have the exact same content but are sent to a different inbox directory for example. This leaves us with 1574 documents (196 sensitive, 1378 non-sensitive documents) to assess. We continue to process our documents by segmenting large documents because LLMs have a context window that limits the number of input tokens; therefore, we perform chunking on large documents at a token-level. Our model families, Llama-2 and Mistral, have a context window of 4096, and we segment our documents at 2048 tokens so we have available tokens for our prompt engineering methods. We find that many documents before chunking is applied are below this threshold, and we only require chunking for 191 documents. Therefore we have 1871 total documents used in inference with our LLMs. To acquire sensitivity predictions for our 1574 unique SARA documents, we collect documents that are segments of a larger document, where if any segment is identified as sensitive, the entire document is regarded as sensitive.

\begin{figure*}[!h]
\begin{center}
\includegraphics[scale=0.3]{figures/pipeline.pdf}
\end{center}
\caption{\label{fig-pipeline}Our experiment pipeline uses SARA from IR datasets, and combines processed documents with different prompt functions. This is given to a generative LLM in a zero-shot fashion, where we obtain generated classifications through natural language that are fit for processing.}
\end{figure*}

\subsection{Tools}
We use the deep learning framework, PyTorch \cite{paszke2019pytorch}, for development in our project. This Pythonic approach allows us to use Python libraries for preprocessing through to evaluation. We use the Hugging Face platform \cite{wolf-etal-2020-transformers}, which hosts open-source models for machine learning, to retrieve the LLMs for our experiment.

\subsection{Prompt Design}
Motivated by previous work (Section \ref{sec:background:prompt_engineering}), we have designed different prompts following prompt engineering techniques. We refined our prompt through extending text classification prompts \cite{kocon2023chatgpt, puri2019zero}. We iteratively identified what context must be provided to classify sensitive personal information within our documents, to establish a sensible base prompt. We establish our base prompt, consisting of an explanation of the problem (identifying sensitive personal information), information that these documents are email messages from Enron, a question we pose instructing the model to answer about the email containing sensitive personal information, and a prefix so the model response is appropriate for processing the classification label. This base prompt is shown by the black text in Figure \ref{fig-input}. We then build on our base prompt using prompt engineering techniques, shown by coloured text in Figure \ref{fig-input}.

We pose our question as a classification task: directly asking the model to choose one of two classes (personal or non-personal), as done in previous work \cite{kocon2023chatgpt, puri2019zero}. Preliminary experiments found using classes ``sensitive", ``non-sensitive" could not be understood effectively, generating explanations about sensitive business affairs; therefore, we use the classes ``personal", ``non-personal". As this question is posed as a direct choice of classes, we used `Answer: [' as a prefix which was a sufficient guide to generate expected responses given instructions to include the attribute in a Python list.

We use the \textbf{context manager prompt engineering pattern} to introduce additional context about the dataset, and what should be treated as sensitive personal information. Background context on what sensitive personal information using statements about Section 40 of the FOIA is included (shown in Figure \ref{fig-input} by blue text); motivated by prompts identifying information exempt under FOIA Exemption 5 \cite{baron2023using}, and from preliminary analysis demonstrating models could explain information about this act. We also use context about the dataset, stating information about the categories of emails within Enron. This context uses explanations of personal information that is purely personal and personal but in a professional context (sensitive email categories), as well as information about non-personal emails (non-sensitive email categories). We explored adding context about personal information, aiming to inform the model about what we wish to protect and reducing confusion about sensitive documents by the model. We also include information about the non-sensitive email categories using Hearst’s Enron annotations. This aims to suggest to the model that it can differentiate emails into multiple sub-categories; for example, identifying a message belongs to the ‘empty with attachment’ category offers more nuance when understanding if emails are personal or non-personal. This additional prompted context aims to expand on using simply generic personal or non-personal keywords as seen in our base prompt.

We use \textbf{few-shot prompting} by presenting examples of sensitive and non-sensitive emails before the message we aim to classify. We cherry-pick our examples, where our sensitive example contains a personal question about an individuals well-being that is not work-related, and our non-sensitive example is about arranging a work-related call. We also ensure these documents are concise such that they fit within the context window yet still convey why they are regarded as sensitive or non-sensitive.

We use the \textbf{chain-of-thought prompt engineering technique}. Related work demonstrates this technique can improve LLMs at complex reasoning tasks. We use ‘thought hops’, designed to invoke and enhance the LLMs’ reasoning capabilities. Following this, we ask our main classification question to the model, aiming to assess if self-reasoning about sensitive personal information before classifying the document improves the classification accuracy. The answer instructions for our three hops are:
\begin{enumerate}
    \item State what personal information is present in the message if any.
    \item You have already identified if personal information could be present within the message, which is shown after the message.
    
    State if the personal information you identified is sensitive personal information that should be protected.
    \item You have already identified if personal information is present within the message, and then if this personal information is sensitive. This reasoning is shown after the message.

    Which one of the attributes: "personal", or "non-personal" describes the following message?
    Always answer in the form of a Python list containing the appropriate attribute.
\end{enumerate}
, where each hop feeds into the next as shown in \cite{fei2023reasoning}.

\begin{table}[!h]
\caption{Prompt strategies and abbreviations.}
\label{table:prompt_abbrev}
\begin{tabular}{ll}
\hline
Prompt Strategy                          & Abbreviation \\ \hline
Text classification                      & Base         \\
Context - sensitive email categories     & SensCat      \\
Context - non-sensitive email categories & NonSensCat   \\
Personal sensitivity context             & SensDesc     \\
Few-shot                                 & FS           \\
Chain-of-Thought Hops                    & CoT          \\ \hline
\end{tabular}
\end{table}

\begin{figure*}
\begin{center}
\includegraphics[scale=0.3]{figures/inputprompt.pdf}
\end{center}
\caption{\label{fig-input}Enron email message and prompting strategies defined by coloured text. Our black text base prompt is extended using personal sensitivity context (blue), sensitive email category context (red), and non-sensitive email category context (green).}
\end{figure*}

\subsection{LLMs}
\label{sec:experiment:LLMs}
We use instruction-tuned LLMs as these are better at generating responses to instructions and questions compared to their corresponding non-instruction counterparts. We are also limited by computational cost; therefore, we use models smaller than state-of-the-art. However, our models are still large, with seven billion parameters, and a larger model with quantisation \cite{frantar2022gptq}. Our techniques aim to show improvements to classification quality, so we hope these techniques will scale to larger models.

We use the popular open-source generative LLMs Llama-2-7b-chat and Mistral-7B-Instruct-v0.2 at full precision as from early work we find these models are insufficient at the classification task when quantised; often hallucinating. With larger models we are required to quantise due to hardware limitations, and we use Mixtral-8x7B-Instruct-v0.1 (GPTQ at 4 bits). These models are often used in benchmarks, where the latest Mistral model’s show performances better than Meta’s Llama-2 models. These models all use the decoder-only architecture. Furthermore, Mixtral uses a mixture-of-experts paradigm \cite{jacobs1991adaptive, shazeer2017outrageously}, which is becoming increasingly popular within generative LLMs and used by GPT-4. 

[We also experiment with FLAN-T5-XL \cite{chung2022scaling}, a generative model with 3 billion parameters that has an encoder-decoder architecture.]

We do not explore different generation hyperparameters within our experiment. We use default generation hyperparameters, but modify sampling to greedy decoding which is deterministic and produces reproducible results. The generation configuration also fixes the number of generated tokens; where we find ten tokens is the minimum number of suitable tokens to generate class names. Our limits on generated tokens reduces the time for inference, and hence overall cost of using the models. Furthermore, for prompts of interest we generate up to a maximum of 150 tokens for analysis of model justification supporting the classifications, and for chain-of-thought hops we generate 60 tokens for each hop before the final classification.

\subsection{Baselines}
We set up simple baselines with scikit-learn to compare our LLMs’ classification performance against. We use baselines stratified random sampling (balanced accuracy of 0.5 demonstrates random predictions for binary classification) and a naive most frequent classifier (which is not useful for identifying sensitive personal information as exempt documents are often the minority class). We use TF-IDF features for the traditional machine learning techniques logistic regression and support vector machines which have been used in previous sensitivity classification \cite{mckechnie2024sara}.

When conducting our statistical tests we treat our $Base$ prompt as a baseline prompting comparison. Our hypothesis states that prompt engineering techniques should improve on baseline prompting classification. Furthermore we treat prompts without few-shot or chain-of-thought augmentation as pseudo-baselines to their related few-shot or chain-of-thought prompts within our statistical tests.

\subsection{Evaluation measures}
In collections where sensitive documents are present, these are the usually the minority class, and this is true for the SARA dataset. As this dataset is imbalanced, and we aim to identify sensitive documents, traditional accuracy is not the best metric due to its bias toward the majority class. Therefore, we select our main metrics as balanced accuracy (BAC) and the F\textsubscript{2}-score. BAC is the average of the proportion of correct predictions in each class, and this ensures that the model is identifying this smaller proportion of sensitive documents which is critical/important. The F\textsubscript{2}-score is a variation of the F-score that considers recall twice as important as precision, and this emphasises value of the model in protecting sensitive documents because the cost of misclassifying a sensitive document as non-sensitive (false negative) is greater than the cost of misclassifying a non-sensitive document as sensitive (false positive). The equation for F\textsubscript{2} is:
\begin{equation*}
F_2\ score = \frac{5 \times Precision \times Recall}{4 \times Precision + Recall} .
\end{equation*}
We also report standard classification measures accuracy, precision, true positive rate (TPR), true negative rate (TNR) and F\textsubscript{1}.

We use McNemar’s non-parametric test \cite{mcnemar1947note} for significance testing. Our data is paired, and our outcomes are binary; therefore, we use McNemar’s to conclude if one prompt has a statistically significant better performance (p < 0.05). McNemar’s is calculated from the contingency tables of the prompt settings we compare. Significant improvements compared to the baseline prompt ($Base$) are denoted by $\dagger$. Additionally, significant improvements to pseudo-baselines compared to augmentation with few-shot or CoT prompting are denoted by $\ddagger$.

[Evaluation strategies used in Analysis].

\subsection{Limitations}
Using these models has a limited context window.
Pre-trained biases.
Less knowledge of UK FOIA – FOIA initially understood focus was American.
Lack of knowledge about the training data.
Emotive language in emails (less of limitation, more of a result).
Prompts can be brittle.
Model size – known GPT4 is big and good.
Language evolution – emails are from 2001.


Code for Experiments 
pipeline

