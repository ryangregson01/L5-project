\section{Experimental Setup}
\begin{figure*}[!h]
\begin{center}
\includegraphics[scale=0.23]{figures/pipeline.pdf}
\end{center}
\caption{\label{fig-pipeline}Our experimental pipeline uses SARA from IR datasets, and combines processed documents with different prompt functions. This is given to a generative LLM in a zero-shot fashion, where we obtain generated classifications through natural language that are fit for processing.}
\end{figure*}

The objective of our experiments are to see how effectively we can classify potentially sensitive documents using generative LLMs, and the impact of prompt engineering strategies for identifying sensitive personal information. To do this we treat the combination of prompt engineering strategies as prompt functions and use natural language to determine our classification predictions as shown by our experimental pipeline in Figure \ref{fig-pipeline}. In our experiment, we seek answers to the following questions:
\begin{itemize}
    \item \textbf{RQ1}: How effective are zero-shot generative LLMs for identifying sensitive personal information within emails when given basic instructions?
    \item \textbf{RQ2}: Does prompt engineering improve the effectiveness of zero-shot generative LLMs for identifying sensitive personal information compared to a prompt with basic instructions?
    \begin{itemize}
    \item RQ2.1 Does the context manager prompt engineering strategy, which introduces context about sensitive personal information, improve the effectiveness of sensitivity classification with generative LLMs?
    \item RQ2.2: What is the effect of few-shot prompting on the effectiveness of sensitivity classification with generative LLMs?
    \item RQ2.3: Does chain-of-thought use reasoning effectively to improve the effectiveness of sensitivity classification with generative LLMs?
    \end{itemize}
    \item \textbf{RQ3}: Does our proposed use of generative LLMs for identifying sensitive personal information within emails improve on existing sensitivity classification methods for sensitive personal information?
\end{itemize}

\subsection{Dataset}
As the dataset for our experiments, we use the SARA collection \cite{mckechnie2024sara}. The SARA collection has many long and unstructured email threads; therefore, we preprocess these documents to create consistency between the documents and remove noise across the collection, giving us more robust input to our LLMs. Our preprocessing involves:
\begin{enumerate}
    \item Removing email headers, as we are interested in personal information found in the body of our messages. Furthermore, email headers results in more tokens, using LLMs' important space in the context window.
    \item Removing email addresses from email thread metadata; replacing {\em@} with a space. This obvious personal identifier inherent to email threads caused model confusion in early experiments. We remove these for identifying sensitive personal information within emails.
    \item Removing all consecutive whitespace characters.
    \item Using simple preprocessing strategies to remove numerical values, punctuation and lowercase our documents \cite{rehurek2011gensim}.
\end{enumerate}

We do not remove any stop words from the documents as this is less representative of the training data seen by pretrained LLMs.

Preprocessing, such as header removal, leads us to remove 128 duplicate documents which have the exact same content but are sent to a different inbox directory for example. This leaves us with 1574 documents (196 sensitive, 1378 non-sensitive documents) to assess. As a sanity check, we extrapolate results for the removed documents using the kept identical document and find no significant changes to our results and concluded trends. We continue to process our documents by segmenting large documents because LLMs have a context window that limits the number of input tokens; therefore, we perform chunking on large documents at a token-level. Our model families, Llama-2 and Mistral, have a context window of 4096, and we segment our documents at 2048 tokens so we have available tokens for our prompt engineering methods. We find that many documents before chunking is applied are below this threshold, and we only require chunking for 191 documents. Overall, we have 1871 total documents used for inference with our LLMs. To acquire sensitivity predictions for our 1574 unique SARA documents, we collect documents that are segments of a larger document, where if any segment is identified as sensitive, the entire document is regarded as sensitive.

\subsection{Tools}
We use the deep learning framework, PyTorch \cite{paszke2019pytorch}, for development in our project. This Pythonic approach allows us to use Python libraries for preprocessing through to evaluation. We use the Hugging Face platform \cite{wolf-etal-2020-transformers}, which hosts open-source models for machine learning, to retrieve the LLMs for our experiment.

\begin{figure*}
\begin{center}
\includegraphics[scale=0.27]{figures/inputprompt.pdf}
\end{center}
\caption{\label{fig-input}An Enron email message and our prompting strategies defined by coloured text. Our black text base prompt is extended using personal sensitivity context (blue), sensitive email category context (red), and non-sensitive email category context (green).}
\end{figure*}

\subsection{Prompt Design}
Motivated by previous work (Section \ref{sec:background:prompt_engineering}), we have designed different prompts following prompt engineering techniques. We refined our prompt through extending a text classification prompt used with generative LLMs \cite{kocon2023chatgpt, puri2019zero}. We identified what instruction and context must be provided to classify sensitive personal information within our documents, to establish a sensible base prompt. We establish our base prompt, consisting of an explanation of the problem (identifying sensitive personal information), information that these documents are email messages from Enron, a question we pose instructing the model to answer about the email containing sensitive personal information, and a prefix so the model response is appropriate for processing the classification label. This base prompt is shown by the black text in Figure \ref{fig-input}. We then build on our base prompt using prompt engineering techniques, shown by coloured text in Figure \ref{fig-input}.

We pose our question as a classification task: directly asking the model to choose one of two classes (personal or non-personal), as this direct class choice is done in previous work \cite{kocon2023chatgpt, puri2019zero}. Preliminary experiments found using classes ``sensitive" and ``non-sensitive" could not be understood effectively, generating explanations about sensitive business affairs; therefore, we use the classes ``personal" and ``non-personal" which is given context from our instruction to determine if the message contains sensitive personal information. As this question is posed as a direct choice of classes, we used `Answer: [' as a prefix which was a sufficient guide to generate expected responses given instructions to include the attribute in a Python list, which is also done in related work as a form of answer engineering \cite{kocon2023chatgpt}.

We use the \textbf{context manager prompt engineering pattern} to introduce additional context about sensitive email categories, and what should be treated as sensitive personal information. We provide background context on what sensitive personal information is using statements about Section 40 of the FOIA (shown in Figure \ref{fig-input} by blue text); motivated by prompts identifying information exempt under FOIA Exemption 5 \cite{baron2023using}, and from preliminary analysis demonstrating models could explain information about this act. We also use context about the dataset, stating information about the categories of emails within Enron. This context uses explanations of personal information that is purely personal and personal but in a professional context (sensitive email categories), as well as information about non-personal emails (non-sensitive email categories). We hypothesise adding context about personal information will inform the model about what we wish to protect and reduce confusion about sensitive documents by the model. We also hypothesise that information about the non-sensitive email categories, using Hearst’s Enron annotations, will suggest to the model that it can differentiate emails into multiple sub-categories; for example, identifying a message belongs to the ‘empty with attachment’ category offers more nuance when understanding if short emails with less surrounding context are personal or non-personal. Overall, this additional prompted context aims to expand on using the simple and generic personal or non-personal keywords as seen in our base prompt.

We use \textbf{few-shot prompting} by presenting examples of sensitive and non-sensitive emails before the message we aim to classify. We use the same two examples for every document. Our sensitive example is purely personal which is the most important type of sensitivity to correctly identify, and our non-personal example is about arranging a work-related call which is common non-sensitive email within the collection. We also ensure these documents are concise such that they fit within the context window yet still convey why they are regarded as sensitive or non-sensitive.

We use the \textbf{chain-of-thought (CoT) prompt engineering technique}, as related work demonstrates this technique can improve LLMs at complex reasoning tasks. Fei et al. demonstrate three-hop CoT \cite{fei2023reasoning}, where `hops' are responses to sub-tasks that are fed back into the LLM to support answering harder questions. We hypothesise that three steps will be useful in our task by first identifying personal information directly from the text, reasoning about this found personal information being sensitive, then finally generating a classification label using this generated self-reasoning. The answer instructions for our three hops are:
\begin{enumerate}
    \item State what personal information is present in the message if any.
    \item You have already identified if personal information could be present within the message, which is shown after the message.
    
    State if the personal information you identified is sensitive personal information that should be protected.
    \item You have already identified if personal information is present within the message, and then if this personal information is sensitive. This reasoning is shown after the message.

    Which one of the attributes: "personal", or "non-personal" describes the following message?
    Always answer in the form of a Python list containing the appropriate attribute.
\end{enumerate}
, where each response is embedded into the next prompt. We also find separate hops are imperative to obtain final sensitivity classifications with our smaller and zero-shot models as requesting reasoning to be finished with a classification in a single instruction results in undesirable responses that cannot be processed; for example, reasoning with no final answer, or hallucinations such as short reasoning followed by generating a new email which demonstrates that the instruction-based LLMs we use can struggle to follow instructions that are less direct and require sequential steps within a single prompt.

\subsubsection{Prompt Abbreviations}
We summarise these prompt strategies in Table \ref{table:prompt_abbrev} below, where multiple strategies used together are defined using addition notation.

\begin{table}[!h]
\caption{Prompt strategies and abbreviations.}
\label{table:prompt_abbrev}
\begin{tabular}{ll}
\hline
Prompt Strategy                          & Abbreviation \\ \hline
Basic Classification Instruction                      & Base         \\
Context - Sensitive Email Categories     & S\_EC      \\
Context - Non-Sensitive Email Categories & NS\_EC   \\
Context - Personal Sensitivity             & PS     \\
Few-shot                                 & FS           \\
Chain-of-Thought Hops                    & CoT          \\ \hline
\end{tabular}
\end{table}

\subsection{LLMs}
\label{sec:experiment:LLMs}
LLMs require GPU resources to efficiently operate. This limitation to computational cost leads us to choose models that are smaller than state-of-the-art. However, our models are still large, with 7 billion parameters for Llama 2 and Mistral, and 46.7 billion parameters for the larger Mixtral model, where we apply quantisation \cite{frantar2022gptq} to 4-bit given our resources. Our prompting techniques aim to show improvements to classification quality, so we hope these techniques will scale to larger models.

We use the popular open-source generative LLMs Llama-2-7B-Chat and Mistral-7B-Instruct-v0.2 at full precision as from early work we find these models are insufficient at the classification task when quantised; often hallucinating. For the larger model requiring quantisation we use Mixtral-8x7B-Instruct-v0.1 with post-training quantization \cite{frantar2022gptq} applied at 4-bits (Mixtral-8x7B-Instruct-v0.1-GPTQ).

In our experiments, we use 2 NVIDIA GeForce RTX 3090 GPUs made available through the University to conduct our experiments. Furthermore, we made inference to each document in a batch of one due to memory constraints.

We do not explore different generation hyperparameters within our experiment. We use default generation hyperparameters, but modify sampling to greedy decoding which is deterministic and produces reproducible results. The generation configuration also fixes the number of generated tokens; where we find ten tokens is the minimum number of suitable tokens to generate our classes. Our limits on generated tokens reduces the time for inference, and hence overall cost of using the models. Furthermore, for prompts of interest we generate up to a maximum of 150 tokens for analysis of model justification supporting the classifications through verbose explanations, and for chain-of-thought hops we generate up to 60 tokens for each hop before the final classification step.

\subsection{Baselines}
We set up simple baselines with scikit-learn \cite{pedregosa2011scikit} to compare our LLMs’ classification performance against. We use baselines stratified random sampling and a naive most frequent classifier which is not useful for identifying sensitive personal information as sensitive documents are often the minority class. We use TF-IDF features for the traditional machine learning techniques logistic regression and support vector machines which have been used in previous sensitivity classification with SARA \cite{mckechnie2024sara}.

When conducting our statistical tests we treat our $Base$ prompt as a baseline prompting comparison. Our hypothesis states that prompt engineering techniques should improve on a baseline prompting classification strategy. Furthermore we treat prompts without few-shot or chain-of-thought augmentation as pseudo-baselines to their related few-shot or chain-of-thought prompts within our statistical tests.

\subsection{Evaluation Measures}
In collections where sensitive documents are present, these are usually the minority class, and this is true for the SARA dataset with 12.4\% of the collection labelled as sensitive. As this dataset is imbalanced, and we aim to identify sensitive documents, traditional accuracy is not the best metric due to its bias toward the majority class. Therefore, we select our main metrics as balanced accuracy (BAC) and the F\textsubscript{2}-score. BAC is the average of the proportion of correct predictions in each class, and this ensures that the model is identifying this smaller proportion of sensitive documents which is critical. Moreover, a BAC of 0.5 demonstrates random predictions for binary classification. The F\textsubscript{2}-score is a variation of the F-score that considers recall twice as important as precision, and this emphasises value of the model in protecting sensitive documents because the cost of misclassifying a sensitive document as non-sensitive (false negative) is greater than the cost of misclassifying a non-sensitive document as sensitive (false positive). We also report standard classification measures accuracy, precision, true positive rate (TPR), true negative rate (TNR) and F\textsubscript{1}.

We use McNemar’s non-parametric test \cite{mcnemar1947note} for significance testing. Our data is paired, and our outcomes are binary; therefore, we use McNemar’s to conclude if one prompt has a statistically significant better performance ($p < 0.05$) given a fixed model. McNemar’s is calculated from the contingency tables of the prompt strategies we compare. Significant improvements compared to the baseline prompt ($Base$) are denoted by $\dagger$. Additionally, significant improvements to pseudo-baselines compared to augmentation with few-shot or CoT prompting are denoted by $\ddagger$.

\subsection{Limitations}
A limitation of our exploration of prompt engineering is that prompts can be brittle and must be crafted carefully to ensure relevant instructions are given to the model. New work aims to optimise prompt instructions to better fit defined tasks given initial base prompting instructions and a small sample of data using DSPy teleprompters \cite{khattab2023dspy}. This strategy may help to discover information useful in prompts that we do not explicitly state in our engineered prompts. However, for complex tasks that require direction and pragmatic understanding, we believe our stated instructions would complement these optimisation approaches to potentially find the best formatted prompt instructions for use in sensitivity identification tasks.

Another limitation is the pre-trained biases and understanding of information that the pretrained LLM is most attuned to. For example, LLM responses initially understood FOIA within American law context, unable to find Section 40, encouraging us to explicitly state United Kingdom within this prompt context. Furthermore, large private collections may have language that is not similar to information seen during model training. For example, SARA is a collection from the early 2000s, and email communication has evolved. Therefore, we suggest that LLMs that experience fair and diverse training will be less limited at identifying sensitive information given appropriate prompts.

Finally, a limitation of using smaller models refrains our exploration of the upper performance of zero-shotting generative LLMs. Research shows that a larger model size results in significant improvements compared to smaller models. Despite this, we believe it is important to explore these smaller models as token digestion and generation using large models have expensive API costs, or require expensive hardware to load these models into memory which may not be feasible.
