Sensitive personal information is prevalent amongst documents within large collections that cannot be opened and made available without manual review to ensure the privacy and security of individuals. It would be beneficial to access the valuable information contained in these collections without infringing on privacy or confidentiality; however, the volume of content to review is too vast. This motivates the need for tools that can automatically identify and remove documents containing sensitive information. We propose a novel use of zero-shot learning with pre-trained LLMs, that generate natural language, to identify sensitive personal information and explore prompt engineering strategies to enhance the performance of these models. We find pre-trained LLMs are not successful at classifying sensitive personal information given basic classification instructions; however, using our prompt engineering strategies, LLMs make statistically significant improvements in identifying sensitive personal information compared to our baseline prompting approach. With our prompting strategies we achieve a balanced accuracy score of 67.96\% on our test collection, with the LLM Mistral, a 14.58\% improvement from basic prompts.
