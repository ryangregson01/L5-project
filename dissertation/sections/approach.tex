\section{Approach}
We discuss our approach of using a public email collection to explore the automatic classification of sensitive personal information in emails. We also discuss our motivations for using the open-source LLMs we choose for our experiment, then formally state our prompting strategies.

\subsection{Sensitive Information in Emails}
Identifying sensitive personal information is an important task, and previous work (discussed in Section \ref{sec:background:sensitivity_detection}) has aimed to classify personal information as it is the most prominent exemption that prevents a document from release in the UK \cite{TNA:16}. Interviews by Iqbal et al. reveal interviewees are often concerned about the intermixing of private conversations and work emails \cite{iqbal2021search}. They showed it is easy to accidentally reveal sensitive personal information within work emails as private information is commonly shared amongst our closest colleagues. Therefore, we explore sensitivity classification of work-related email documents. Automatic sensitivity classifiers could efficiently and safely inspect emails, then remove any containing sensitive personal information. Accessing email collections would also be useful for archives interested in storing information from government, universities and businesses for future research. However, currently email collections are rarely regularly collected by archives due to privacy concerns and the volume of documents in an email collection \cite{TFTAEA:18}. Improved sensitivity identification methods will allow archivists to open email collections while respecting the privacy of email authors, motivating our exploration of identifying sensitive personal information within emails using new techniques.

We use a public email collection, Enron \cite{klimt2004enron}, which was released during a legal investigation of the company’s collapse in 2001. This collection contains 619,446 company email messages. A subset of 1702 email threads was annotated by students at UC Berkeley for relevance to eight coarse genres present in work emails \cite{hearst2005teaching}. These genres include Company Business and Strategy, Purely Personal, Personal but in Professional Context, Logistic Arrangements (such as meeting scheduling and technical support), Employment Arrangements, Document Collaboration, Empty Message (sending attachment), and Empty message (forwarded messages). Consequently, we believe our findings can be applied to other email collections in professional settings.

McKechnie et al. produce SARA \cite{mckechnie2024sara}, a collection of sensitivity-aware relevance assessments for UC Berkeley’s Enron subset. McKechnie et al. use the coarse genres `purely personal' which contain emails with no relation to work and discusses personal affairs, and `personal but in a professional context' which contains emails that do have relation to work being done at Enron but discusses individuals' quality of work and personal opinions about employee treatment, to produce sensitivity labels for sensitive personal information. SARA’s 1702 documents have 211 sensitive documents, 1491 non-sensitive documents, and is accessible through the python package `IR datasets' \cite{macavaney2021simplified}.

\subsection{Model Choice}
\label{sec:approach:ModelChoice}
Pre-trained LLMs are known to be effective zero-shot learners \cite{kojima2022large}. The benefit of zero-shot learning means we can have effective inference without the cost of fine-tuning. In our classification task, we use natural language to command the model to generate a class of interest that was not the primary focus during the pre-training stage. Our investigation aims to evaluate if LLMs can successfully generate the sensitive or non-sensitive class for a given document. Therefore, we investigate if LLMs are effective without any training as this suggests these pre-trained generative models will be useful in other sensitive domains. A limitation when using pretrained LLMs is that their training data will have included biases that are unwanted for assessing potentially sensitive documents. However, the large training process that comes with language models of this size means it is difficult to ensure all training is completed with unbiased data.

Recent generative LLMs, such as OpenAI’s GPT-4 model \cite{openai2023gpt}, show great advancements on NLP benchmarks, showcasing the advanced reasoning capabilities of LLMs. However, accessing OpenAI’s newest and high-performance models have a cost barrier. We instead utilise readily accessible models, which are open-source, and can be conveniently downloaded from Hugging Face \cite{wolf-etal-2020-transformers} in our experiments. By using open-source LLMs we aim for easy extensibility by other researchers and affordability for everyone implementing sensitivity classification. Furthermore, many modern generative LLMs, such as GPT-4, use a decoder-only transformer architecture. Therefore, we have explored open-source decoder-only LLMs, including Meta’s Llama 2 \cite{touvron2023llama}, and the Mistral models (Mistral \cite{jiang2023mistral} and Mixtral \cite{jiang2024mixtral}). These models are often used in benchmarks, where the latest Mistral models show performances better than Meta’s Llama 2 models. Furthermore, Mixtral uses a mixture-of-experts paradigm \cite{jacobs1991adaptive, shazeer2017outrageously}, which is becoming increasingly popular within generative LLMs and used by GPT-4.

We use versions of these LLMs that have been further instruction fine-tuned as these are better at generating responses to instructions and questions compared to their corresponding non-instruction counterparts \cite{wei2021finetuned}. Prompt engineering techniques are additional focused instructions; therefore, these models can respond more effectively given contextual information and queries about our sensitivity classification task.

Preliminary analysis exploring the knowledge these pre-trained LLMs possess regarding sensitive personal information is demonstrated through a concise question answering experiment. To assess our models’ understanding, we posed queries such as ‘What does sensitive personal information mean?’, and ‘Describe UK FOIA Section 40’. The models effectively generated responses explaining the laws and regulations designed to safeguard sensitive personal information, such as the GDPR in Europe. Models also generated examples of protected attributes, including financial and health information, biometric and location data, as well as personal characteristics like race and religion (Appendix Table \ref{table:model_justify} \& Table \ref{table:model_justify2}). The models' ability to produce contextually appropriate information based on their training data indicates a foundational understanding of sensitive personal information. This understanding gives us confidence in their application for our purposes of sensitivity review.

\subsection{Prompting Strategies for Sensitive Personal Information}
Prompt engineering is used to steer the model into being suitable for a task that is not necessarily the goal of initial fine-tuning objectives. Pretrained LLMs possess an extensive (English) vocabulary, and research indicates that these models, when appropriately prompted, can effectively identify and categorise documents in tasks like sentiment analysis \cite{kocon2023chatgpt, krugmann2024sentiment}. Moreover, prompts can influence the role or character the model assumes.

We design our prompts to instruct the model to use its understanding of sensitive personal information and directly apply this knowledge to identifying sensitive information within the email message. This allows the model to classify a document as sensitive or non-sensitive, similar to how a human reviewer would conduct a sensitivity review to protect sensitive content. Additionally, we can apply extra details about the task and email collection in the prompt to provide context to the model.

Emails in our chosen Enron collection are delivered from company email addresses where there is an expectation of professionalism. Additionally, SARA categorises sensitive emails as purely personal and personal but in a professional context. Describing these categories puts the sensitive information to identify in a suitable context, which helps explain our task to the LLMs. As well as simply providing context, we can also use few-shot prompting which provides examples of email messages and their annotation, showcasing sensitive personal information that has already been identified. We also apply more complex reasoning via chain-of-thought prompts, which allows the model to describe personal information within the message, and reason if it is sensitive, before classifying the message as sensitive or non-sensitive. This intermediary step serves as self-generated rationale, facilitating reasoning before the generated classification prediction.

