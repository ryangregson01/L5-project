In this paper, we proposed novel prompt engineering strategies to improve the zero-shot performance of open-source generative LLMs in sensitivity classification. We use instructions to influence LLMs understanding of sensitive personal information within emails by introducing context about sensitive and non-sensitive email categories, and a personal sensitivity definition ($S\_EC$, $NS\_EC$, $PS$), few-shot prompting ($FS$), and chain-of-thought prompting ($CoT$). Our experiments on the SARA dataset found that basic classification prompting strategies ($Base$) were not sufficient at classifying sensitive personal information. We found using prompt engineering did improve the effectiveness of classifying sensitive personal information with generative LLMs. All of our prompting strategies introducing context within the system prompt made some significant improvements (according to McNemarâ€™s test) compared to our $Base$ prompt strategy. Furthermore, we found that few-shot prompting made significant improvements for Mistral; however, our proposed $CoT$ strategy was ineffective. The best performing model and prompt was Mistral with context about sensitive email categories, sensitive personal information laws and few-shot examples ($S\_EC+PS+FS$). Furthermore, Mistral-$S\_EC+PS+FS$ as a sensitivity classifier performed competitively against existing machine learning strategies. We conclude generative LLMs do not outperform existing strategies; however, they also do not require training data. We believe this zero-shot learning capability, enhanced by prompts, will be useful in adapting to and managing new sensitivities. Zero-shot models can also be ethically advantageous because we do not require collection of sensitive data for training.

\subsection{Future Work}
From our evaluation, we know our prompting strategies do not fully cover every context required to identify all sensitive personal information. As well as this, refining these prompts to include appropriate context can be time-consuming and difficult. A new framework, DSPy \cite{khattab2023dspy}, claims to replace manual prompt engineering using auto-tuned prompts. It could be useful to investigate if an optimised basic instruction prompt was more effective at sensitivity classification than our manually engineered prompts. It would also be useful to explore if using our manually engineered instructions as a starting point for prompt optimisation produces more effective prompts for identifying sensitive information. Furthermore, some models did not respond well to our few-shot examples, and DSPy is able to generate synthetic examples which may mitigate the confusion models had in our experiments when using few-shot prompting.

These instruction-based generative LLMs can be used to answer queries. Recent exploration of sensitivity-aware search \cite{mcdonald2021search} aims to identify relevant and non-sensitive documents. These LLMs could be prompted with appropriate system instructions to both protect sensitive information and score relevant documents given a query.

\vspace{0.4cm}
{\bf Acknowledgments.}
I would like to thank my supervisor, Dr. Graham McDonald, for his valuable comments and advice throughout the year.


